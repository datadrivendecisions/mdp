{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: MDP Assignment 2\n",
        "author: Witek ten Hove\n",
        "format: html\n",
        "editor: visual\n",
        "---"
      ],
      "id": "b31797ff"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Service Rate Control\n",
        "\n",
        "Consider a discrete-time single-server queueing system that is observed every $\\eta > 0$ units of time. The controller makes decisions at times $0, \\eta, 2\\eta, \\dots$. Jobs arrive following a Poisson distribution with a rate of 1.5 jobs per period of length $\\eta$. The system has a finite capacity of 8 units, meaning if arriving jobs cause the system content to exceed 8 units, the excess jobs do not enter the system and are lost.\n",
        "\n",
        "At each decision epoch, the controller observes the number of jobs in the system and selects the service rate from a set of probability distributions indexed by elements of the set $B = \\{0, 1, 2\\}$. For each $b \\in B$, let $f_b(n)$ denote the probability of $n$ service completions within a period of length $\\eta$ where:\n",
        "\n",
        "-   $f_0(1) = 0.8$, $f_0(2) = 0.2$\n",
        "-   $f_1(1) = 0.5$, $f_1(2) = 0.5$\n",
        "-   $f_2(1) = 0.2$, $f_2(2) = 0.8$\n",
        "\n",
        "The stationary reward structure consists of four components:\n",
        "\n",
        "1.  A constant reward $R = 5$ for every completed service.\n",
        "\n",
        "2.  An expected holding cost $h(s) = 2s$ per period when there are $s$ jobs in the system.\n",
        "\n",
        "3.  A fixed cost $K = 3$ for changing the service rate.\n",
        "\n",
        "4.  A per-period cost $d(b)$ for using service rate $b$, where:\n",
        "\n",
        "    -   $d(0) = 0$\n",
        "\n",
        "    -   $d(1) = 2$\n",
        "\n",
        "    -   $d(2) = 5$\n",
        "\n",
        "We are tasked with determining a minimum-cost service rate control policy.\n",
        "\n",
        "### (a) Problem Formulation\n",
        "\n",
        "-   Formulate the problem as an infinite horizon Markov decision process (MDP).\n",
        "-   Choose the optimality criterion: average costs or discounted costs, and provide justification.\n",
        "-   Develop the model and algorithm to compute the optimal policies and value.\n",
        "    -   Write your own code for the algorithm (do not use existing MDP libraries).\n",
        "\n",
        "### Please Report:\n",
        "\n",
        "-   Model description\n",
        "-   Your choice of optimality criterion, including motivation\n",
        "-   Solution algorithm (including motivation)\n",
        "-   Numerical results and a discussion of those\n",
        "\n",
        "### (b) Additional Constraint\n",
        "\n",
        "Now, suppose we require that the server may work at service rate $b = 2$ for at most 25% of the time. Model and solve this adjusted problem.\n",
        "\n",
        "![](mm1.png)\n",
        "\n",
        "## Tests\n"
      ],
      "id": "2f4aba5a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "def poisson_function(lam, k):\n",
        "    return (lam**k) * np.exp(-lam) / math.factorial(k)\n",
        " \n",
        "## Test the poisson function\n",
        "\n",
        "assert poisson_function(1.5, 0) == 0.22313016014842982\n",
        "\n",
        "## Plot the poisson function\n",
        "\n",
        "x = np.arange(0, 10, 1)\n",
        "plt.plot(x, [poisson_function(1.5, i) for i in x])\n",
        "plt.show()"
      ],
      "id": "c63048da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulation\n"
      ],
      "id": "958a1e7b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import simpy\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters for the queue\n",
        "RANDOM_SEED = 42\n",
        "ARRIVAL_RATE = 1.5  # lambda (arrival rate per time unit)\n",
        "SIM_TIME = 100  # total simulation time (in minutes)\n",
        "BUFFER_CAPACITY = 8  # buffer can hold up to 8 items\n",
        "\n",
        "# Data tracking for plotting\n",
        "arrivals_per_interval = []  # List to track arrivals in each minute\n",
        "buffer_state_per_interval = []  # List to track buffer state at the end of each minute\n",
        "processed_per_interval = []  # List to track processed items during each minute\n",
        "current_interval = 0\n",
        "arrivals_this_interval = 0\n",
        "processed_this_interval = 0\n",
        "\n",
        "# Define the service time distribution\n",
        "def service_time():\n",
        "    return random.choices([1, 2], [0.5, 0.5])[0]  # 50% chance of 1 or 2 units of time\n",
        "\n",
        "# Customer arrival process\n",
        "def customer(env, name, server):\n",
        "    global current_interval, arrivals_this_interval, processed_this_interval\n",
        "\n",
        "    # Track arrivals in the current interval\n",
        "    if int(env.now) > current_interval:\n",
        "        # Store the number of arrivals and processed items in the previous minute\n",
        "        arrivals_per_interval.append(arrivals_this_interval)\n",
        "        processed_per_interval.append(-processed_this_interval)  # Store as negative to indicate processing\n",
        "        # Store the net buffer state (arrivals - processed) at the end of the interval\n",
        "        buffer_state_per_interval.append(min(buffer_state_per_interval[-1]+arrivals_this_interval-processed_this_interval,8))\n",
        "        \n",
        "        # Update to the next interval and reset the counters\n",
        "        current_interval = int(env.now)\n",
        "        arrivals_this_interval = 0\n",
        "        processed_this_interval = 0\n",
        "\n",
        "    # Increment arrivals for the current interval\n",
        "    arrivals_this_interval += 1\n",
        "\n",
        "    # Check if buffer has space\n",
        "    if len(server.queue) + len(server.users) < BUFFER_CAPACITY:\n",
        "        with server.request() as request:\n",
        "            yield request\n",
        "            service_duration = service_time()\n",
        "            yield env.timeout(service_duration)\n",
        "            processed_this_interval += 1\n",
        "    else:\n",
        "        # If buffer is full, discard the arrival\n",
        "        print(f\"{name} discarded due to full buffer at time {env.now:.2f}\")\n",
        "\n",
        "# Process generating customers\n",
        "def source(env, server):\n",
        "    i = 0\n",
        "    while True:\n",
        "        interarrival_time = random.expovariate(ARRIVAL_RATE)\n",
        "        yield env.timeout(interarrival_time)\n",
        "        i += 1\n",
        "        env.process(customer(env, f'Customer {i}', server))\n",
        "\n",
        "# Setup and start the simulation\n",
        "print('M/G/1 queue simulation with Poisson arrivals, discrete service times, and a buffer')\n",
        "random.seed(RANDOM_SEED)\n",
        "env = simpy.Environment()\n",
        "\n",
        "# Server with a single resource (1 server)\n",
        "server = simpy.Resource(env, capacity=1)\n",
        "\n",
        "# Initialize the buffer state (starting at 0)\n",
        "buffer_state_per_interval.append(0)\n",
        "\n",
        "# Start the arrival of customers\n",
        "env.process(source(env, server))\n",
        "\n",
        "# Run the simulation\n",
        "env.run(until=SIM_TIME)\n",
        "\n",
        "# Append the last interval's data\n",
        "arrivals_per_interval.append(arrivals_this_interval)\n",
        "processed_per_interval.append(-processed_this_interval)\n",
        "buffer_state_per_interval.append(min(buffer_state_per_interval[-1]+arrivals_this_interval-processed_this_interval,8))\n",
        "print(len(arrivals_per_interval), len(processed_per_interval), len(buffer_state_per_interval))\n",
        "print(buffer_state_per_interval)\n",
        "\n",
        "# Generate a list of time intervals for the x-axis\n",
        "interval_times = list(range(len(arrivals_per_interval)))\n",
        "\n",
        "# Plotting the tracked data\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot number of arrivals per minute\n",
        "plt.step(interval_times, arrivals_per_interval, where='mid', label='Arrivals per Minute Interval', color='blue')\n",
        "\n",
        "# Plot buffer state at the end of each interval\n",
        "plt.step(interval_times, buffer_state_per_interval[1:], where='mid', label='Net Buffer State', color='red')\n",
        "\n",
        "# Plot processed items as negative numbers\n",
        "plt.step(interval_times, processed_per_interval, where='mid', label='Processed per Minute Interval (Negative)', color='green')\n",
        "\n",
        "plt.xlabel('Time (minutes)')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Arrivals, processed items, and buffer state per time unit in M/G/1 queue simulation')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "id": "e796299b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution exercise 3.1 from notes G. Koole\n",
        "\n",
        "**Exercise 3.1** Consider a Markov chain with $X = \\{1, 2, 3, 4\\}$,\n",
        "\n",
        "$$\n",
        "P = \\begin{pmatrix}\n",
        "0 & 1 & 0 & 0 \\\\\n",
        "0 & 0 & 1 & 0 \\\\\n",
        "0 & 0 & 0 & 1 \\\\\n",
        "\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0\n",
        "\\end{pmatrix},\n",
        "$$\n",
        "\n",
        "and $\\pi_0 = (1, 0, 0, 0)$.\n",
        "\n",
        "a.  Compute by hand $\\pi_t$ for $t \\leq 6$.\n",
        "\n",
        "b.  Compute using a suitable tool (for example Maple or Excel) $\\pi_t$ for $t = 10, 20, 30$.\n",
        "\n",
        "c.  Compute by hand $\\pi_\\ast$.\n",
        "\n",
        "a\\.\n",
        "\n",
        "![](images/koole1.jpeg)\n"
      ],
      "id": "3ee6797d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "def calc_probs(pi_t, P, t):\n",
        "  P = np.linalg.matrix_power(P, t)\n",
        "  return np.dot(pi_t, P)\n",
        "  \n",
        "pi_0 = np.array([1,0,0,0])\n",
        "P = np.array([[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [1/3, 1/3, 1/3, 0]])\n",
        "\n",
        "print(P)\n",
        "pi_t = pi_0\n",
        "for t in range (1,7):\n",
        "  pi_t = calc_probs(pi_0, P, t)\n",
        "  print(f'pi_{t} = {pi_t}')"
      ],
      "id": "d561463b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b\\.\n"
      ],
      "id": "e6acdb16"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for t in [10, 20, 30]:\n",
        "  pi_t = calc_probs(pi_0, P, t)\n",
        "  print(f'pi_{t} = {pi_t}')"
      ],
      "id": "5587b80d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c\\.\n",
        "\n",
        "::: {#fig-elephants layout-ncol=2}\n",
        "\n",
        "![Page 1](images/koole2.jpeg){#fig-page1}\n",
        "\n",
        "![Page 2](images/koole3.jpeg){#fig-page2}\n",
        "\n",
        "Solution to Exercise 3.1c.\n",
        ":::\n",
        "\n",
        "**Assumption 2.1.1**  \n",
        "There exists a state, $\\Delta \\in S$ with the following properties:\n",
        "\n",
        "- $\\Delta$ is an absorbing, zero reward state under any Markov policy, i.e., $A(\\Delta) = \\{0\\}$,\n",
        "  $p_{\\Delta, \\Delta}(0) = 1$, $r_{\\Delta}(0) = 0$, and $M(\\Delta) = 1$;\n",
        "  \n",
        "- For all $i \\in S$, $a \\in A(i)$, and some constant $\\gamma \\in (0, 1)$,\n",
        "  $$\n",
        "  \\sum_{j \\neq \\Delta} p_{ij}(a) M(j) \\leq \\gamma M(i).\n",
        "  $$\n",
        "\n",
        "Moreover,\n",
        "\n",
        "- Let $r := \\sup_{a \\in A(i)} \\frac{|r_i(a)|}{M(i)} < \\infty$.\n",
        "\n",
        "To understand this mathematical statement, let's break down each part using simple concepts and then move on to building examples in Python.\n",
        "\n",
        "### Explanation of Each Part\n",
        "\n",
        "1. **State $\\Delta$ as a Special State**:\n",
        "   - We assume there exists a particular state, denoted as $\\Delta$, which is part of the state space $S$.\n",
        "   - $\\Delta$ is an *absorbing* state. This means that once the system enters $\\Delta$, it remains there forever (like a \"sink\" state in a Markov chain).\n",
        "\n",
        "2. **Absorbing, Zero Reward State**:\n",
        "   - In state $\\Delta$, the only available action has a reward of zero. This means the system has no incentive to enter or stay in this state.\n",
        "   - Notationally, this is expressed as:\n",
        "     - $A(\\Delta) = \\{0\\}$: The only action in $\\Delta$ is zero.\n",
        "     - $p_{\\Delta,\\Delta}(0) = 1$: With probability 1, the system stays in $\\Delta$ after choosing action 0.\n",
        "     - $r_{\\Delta}(0) = 0$: The reward in $\\Delta$ for action 0 is zero.\n",
        "     - $M(\\Delta) = 1$: This could represent the importance or weight of this state, set to 1 for simplicity.\n",
        "\n",
        "3. **Condition on Transition Probabilities**:\n",
        "   - For any state $i$ (not equal to $\\Delta$) and any action $a$ available in that state, the probability-weighted sum of transitions to all other states $j \\neq \\Delta$ (scaled by $M(j)$) is bounded above by $\\gamma M(i)$.\n",
        "   - Here, $\\gamma$ is a constant less than 1, which ensures that states move toward $\\Delta$ over time in a weighted sense.\n",
        "\n",
        "4. **Bounded Rewards Condition**:\n",
        "   - The reward function is bounded. Specifically, the highest possible reward, scaled by $M(i)$, across all states and actions is finite.\n",
        "\n",
        "### Python Examples\n",
        "\n",
        "Let’s create Python code that constructs three simple examples of systems that satisfy the above conditions.\n",
        "\n",
        "#### Example 1: Simple Markov Chain with an Absorbing State\n",
        "\n",
        "In this example, we’ll set up a Markov chain with three states where state 2 ($\\Delta$) is absorbing.\n"
      ],
      "id": "4e66a659"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example 1: Simple Markov Chain with absorbing state ∆\n",
        "\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "# Define states and actions\n",
        "states = [0, 1, 2]  # State 2 is the absorbing state (∆)\n",
        "actions = [0, 1]  # Action set, with only action 0 in state 2\n",
        "\n",
        "# Transition probabilities\n",
        "# Format: P[state][action][next_state]\n",
        "P = {\n",
        "    0: {0: [0.7, 0.3, 0.0], 1: [0.4, 0.6, 0.0]},  # State 0 transitions\n",
        "    1: {0: [0.2, 0.7, 0.1], 1: [0.1, 0.5, 0.4]},  # State 1 transitions\n",
        "    2: {0: [0.0, 0.0, 1.0]}                       # State 2 is absorbing\n",
        "}\n",
        "\n",
        "# Rewards for each action in each state\n",
        "R = {\n",
        "    0: {0: 5, 1: 3},  # State 0 rewards\n",
        "    1: {0: 4, 1: 2},  # State 1 rewards\n",
        "    2: {0: 0}          # State 2 has zero reward (absorbing)\n",
        "}\n",
        "\n",
        "# State importance/weight function M\n",
        "M = [0.5, 1.0, 1.0]  # State 2 is absorbing, so its importance is set to 1"
      ],
      "id": "3f3e7ccc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{mermaid}\n",
        "graph LR\n",
        "    %% Define states\n",
        "    S0[\"State 0\"]\n",
        "    S1[\"State 1\"]\n",
        "    S2[\"State 2 (Absorbing)\"]\n",
        "    \n",
        "    %% Transitions from State 0\n",
        "    S0 -->|a0: p=0.7, r=5| S0\n",
        "    S0 -->|a0: p=0.3, r=5| S1\n",
        "    S0 -->|a1: p=0.4, r=3| S0\n",
        "    S0 -->|a1: p=0.6, r=3| S1\n",
        "    \n",
        "    %% Transitions from State 1\n",
        "    S1 -->|a0: p=0.2, r=4| S0\n",
        "    S1 -->|a0: p=0.7, r=4| S1\n",
        "    S1 -->|a0: p=0.1, r=4| S2\n",
        "    S1 -->|a1: p=0.1, r=2| S0\n",
        "    S1 -->|a1: p=0.5, r=2| S1\n",
        "    S1 -->|a1: p=0.4, r=2| S2\n",
        "    \n",
        "    %% Transition for Absorbing State 2\n",
        "    S2 -->|a0: p=1.0, r=0| S2\n",
        "```\n",
        "\n",
        "\n",
        "#### Example 2: Markov Chain with Probabilistic Transition Toward the Absorbing State\n",
        "\n",
        "This example shows a Markov chain where each state has a probability of transitioning toward the absorbing state $\\Delta = 2$.\n"
      ],
      "id": "baae4be7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example 2: Markov Chain with probabilistic transitions towards absorbing state\n",
        "\n",
        "# Define states and actions\n",
        "states = [0, 1, 2]\n",
        "actions = [0, 1]\n",
        "\n",
        "# Transition probabilities favor moving towards state 2 (∆)\n",
        "P = {\n",
        "    0: {0: [0.6, 0.3, 0.1], 1: [0.3, 0.4, 0.3]},\n",
        "    1: {0: [0.3, 0.5, 0.2], 1: [0.2, 0.3, 0.5]},\n",
        "    2: {0: [0.0, 0.0, 1.0]}  # Absorbing state\n",
        "}\n",
        "\n",
        "# Rewards, again with zero reward in absorbing state\n",
        "R = {\n",
        "    0: {0: 6, 1: 4},\n",
        "    1: {0: 3, 1: 5},\n",
        "    2: {0: 0}\n",
        "}\n",
        "\n",
        "# State importance/weight function M\n",
        "M = [1.0, 1.5, 1.0]"
      ],
      "id": "efeea6c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{mermaid}\n",
        "graph LR\n",
        "    %% Define states\n",
        "    S0[\"State 0\"]\n",
        "    S1[\"State 1\"]\n",
        "    S2[\"State 2 (Absorbing)\"]\n",
        "    \n",
        "    %% Transitions from State 0\n",
        "    S0 -->|a0: p=0.6, r=6| S0\n",
        "    S0 -->|a0: p=0.3, r=6| S1\n",
        "    S0 -->|a0: p=0.1, r=6| S2\n",
        "    S0 -->|a1: p=0.3, r=4| S0\n",
        "    S0 -->|a1: p=0.4, r=4| S1\n",
        "    S0 -->|a1: p=0.3, r=4| S2\n",
        "    \n",
        "    %% Transitions from State 1\n",
        "    S1 -->|a0: p=0.3, r=3| S0\n",
        "    S1 -->|a0: p=0.5, r=3| S1\n",
        "    S1 -->|a0: p=0.2, r=3| S2\n",
        "    S1 -->|a1: p=0.2, r=5| S0\n",
        "    S1 -->|a1: p=0.3, r=5| S1\n",
        "    S1 -->|a1: p=0.5, r=5| S2\n",
        "    \n",
        "    %% Transition for Absorbing State 2\n",
        "    S2 -->|a0: p=1.0, r=0| S2\n",
        "```\n",
        "\n",
        "\n",
        "#### Example 3: A System with Different Rewards and Cost Structure\n",
        "\n",
        "Here’s a variant with different rewards and where the absorbing state is reached only after several transitions.\n"
      ],
      "id": "8ac2c760"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example 3: Complex system with delayed transition to absorbing state\n",
        "\n",
        "# Define states and actions\n",
        "states = [0, 1, 2]\n",
        "actions = [0, 1]\n",
        "\n",
        "# Transition probabilities with a focus on eventually reaching the absorbing state\n",
        "P = {\n",
        "    0: {0: [0.5, 0.5, 0.0], 1: [0.3, 0.6, 0.1]},\n",
        "    1: {0: [0.2, 0.4, 0.4], 1: [0.1, 0.4, 0.5]},\n",
        "    2: {0: [0.0, 0.0, 1.0]}  # Absorbing state\n",
        "}\n",
        "\n",
        "# Rewards with varying values but zero reward in state 2\n",
        "R = {\n",
        "    0: {0: 7, 1: 3},\n",
        "    1: {0: 2, 1: 6},\n",
        "    2: {0: 0}\n",
        "}\n",
        "\n",
        "# State importance/weight function M, with a finite bound\n",
        "M = [1.0, 0.8, 1.0]"
      ],
      "id": "c586398d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{mermaid}\n",
        "graph LR\n",
        "    %% Define states\n",
        "    S0[\"State 0\"]\n",
        "    S1[\"State 1\"]\n",
        "    S2[\"State 2 (Absorbing)\"]\n",
        "    \n",
        "    %% Transitions from State 0\n",
        "    S0 -->|a0: p=0.5, r=7| S0\n",
        "    S0 -->|a0: p=0.5, r=7| S1\n",
        "    S0 -->|a1: p=0.3, r=3| S0\n",
        "    S0 -->|a1: p=0.6, r=3| S1\n",
        "    S0 -->|a1: p=0.1, r=3| S2\n",
        "    \n",
        "    %% Transitions from State 1\n",
        "    S1 -->|a0: p=0.2, r=2| S0\n",
        "    S1 -->|a0: p=0.4, r=2| S1\n",
        "    S1 -->|a0: p=0.4, r=2| S2\n",
        "    S1 -->|a1: p=0.1, r=6| S0\n",
        "    S1 -->|a1: p=0.4, r=6| S1\n",
        "    S1 -->|a1: p=0.5, r=6| S2\n",
        "    \n",
        "    %% Transition for Absorbing State 2\n",
        "    S2 -->|a0: p=1.0, r=0| S2\n",
        "```\n",
        "\n",
        "\n",
        "### Explanation of Code Structure\n",
        "- **States and Actions**: Each state and action pair has transition probabilities and rewards.\n",
        "- **Transition Probabilities (`P`)**: Probabilities of moving from one state to another based on chosen actions, with state 2 absorbing.\n",
        "- **Rewards (`R`)**: Each action has a reward in each state, with zero reward in the absorbing state.\n",
        "- **Importance (`M`)**: The weights or importance levels of each state, helping to satisfy the bounded reward condition.\n",
        "\n",
        "\n",
        "**Lemma 2.1.1**  \n",
        "Suppose that Assumption 2.1.1 holds. Then, for all Markov policies $\\sigma$, the following statements hold:\n",
        "\n",
        "- **1. Probability Bound for Remaining Time Before Absorption**:  \n",
        "  $$\n",
        "  P^\\sigma_i \\{\\tau_\\Delta > n\\} \\leq \\gamma^n M(i), \\quad i \\in S, \\quad n = 1, 2, \\dots\n",
        "  $$\n",
        "  where $\\tau_\\Delta = \\min \\{ n \\geq 1 \\mid X_n = \\Delta \\}$.  \n",
        "  This means that the probability of not reaching the absorbing state $\\Delta$ by step $n$ decays at a rate proportional to $\\gamma^n$ and is scaled by $M(i)$, the importance of the initial state $i$.\n",
        "\n",
        "- **2. Expected Time to Absorption**:  \n",
        "  $$\n",
        "  E^\\sigma_i [\\tau_\\Delta] = \\sum_{n \\geq 0} P^\\sigma_i \\{\\tau_\\Delta > n\\} \\leq \\frac{M(i)}{1 - \\gamma}\n",
        "  $$\n",
        "  The expected time to reach the absorbing state $\\Delta$ (starting from state $i$) is bounded by $\\frac{M(i)}{1 - \\gamma}$, ensuring that the process reaches $\\Delta$ within a finite expected time.\n",
        "\n",
        "- **3. Bound on Expected Reward at Each Step Before Absorption**:  \n",
        "  $$\n",
        "  E^\\sigma_i \\left[ \\left| r_{X_n}(A_n) \\right| \\right] \\leq r \\cdot \\gamma^n M(i)\n",
        "  $$\n",
        "  This inequality provides a bound on the expected absolute reward at step $n$, showing that it decays at a rate of $\\gamma^n$, scaled by $M(i)$ and a constant $r$ (the maximum possible reward per unit of importance).\n",
        "\n",
        "- **4. Bound on Total Expected Reward Until Absorption**:  \n",
        "  $$\n",
        "  E^\\sigma_i \\left[ \\sum_{n=0}^{\\tau_\\Delta} \\left| r_{X_n}(A_n) \\right| \\right] \\leq \\frac{r}{1 - \\gamma} M(i), \\quad i \\in S\n",
        "  $$\n",
        "  This final bound indicates that the total expected reward accumulated from the starting state $i$ until reaching $\\Delta$ is finite and depends on $M(i)$, $r$, and $\\gamma$. The bound ensures that the rewards do not accumulate indefinitely, given $\\gamma < 1$.\n",
        "\n",
        "\n",
        "\n",
        "Let's break down each part of Lemma 2.1.1, which provides results about certain expectations and probabilities under a Markov policy, given that Assumption 2.1.1 holds.\n",
        "\n",
        "---\n",
        "\n",
        "### Context and Key Terms\n",
        "\n",
        "1. **Assumption 2.1.1**: This assumption specifies that there exists an absorbing state $\\Delta$ in the state space $S$, with certain properties related to transition probabilities, rewards, and importance weights.\n",
        "   \n",
        "2. **Markov Policy** $\\sigma$: A policy that decides actions based on the current state without considering previous history.\n",
        "\n",
        "3. **Absorbing Time $\\tau_\\Delta$**: This is defined as $\\tau_\\Delta = \\min \\{ n \\geq 1 | X_n = \\Delta \\}$, meaning it is the first time (after at least one step) that the process reaches the absorbing state $\\Delta$.\n",
        "\n",
        "---\n",
        "\n",
        "### Statement of Lemma 2.1.1\n",
        "\n",
        "Under Assumption 2.1.1, the lemma asserts the following results:\n",
        "\n",
        "1. **Probability Bound for Remaining Time Before Absorption**:\n",
        "   $$\n",
        "   P^\\sigma_i \\{\\tau_\\Delta > n\\} \\leq \\gamma^n M(i), \\quad i \\in S, \\quad n = 1, 2, \\dots\n",
        "   $$\n",
        "   - **Interpretation**: This part states that the probability of staying outside the absorbing state $\\Delta$ for more than $n$ steps decays at an exponential rate, $\\gamma^n$, and is scaled by the importance weight $M(i)$ of the initial state $i$.\n",
        "   - This result tells us that the probability of not reaching the absorbing state decreases rapidly over time, due to the $\\gamma^n$ term (where $\\gamma < 1$).\n",
        "\n",
        "2. **Expected Time to Absorption**:\n",
        "   $$\n",
        "   E^\\sigma_i [\\tau_\\Delta] = \\sum_{n \\geq 0} P^\\sigma_i \\{\\tau_\\Delta > n\\} \\leq \\frac{M(i)}{1 - \\gamma}\n",
        "   $$\n",
        "   - **Interpretation**: The expected time to reach the absorbing state $\\Delta$ (starting from state $i$) is bounded above by $\\frac{M(i)}{1 - \\gamma}$.\n",
        "   - This bound implies that, on average, the process reaches the absorbing state fairly quickly because $\\gamma$ is less than 1, causing the expected time to be finite.\n",
        "\n",
        "3. **Bound on Expected Reward at Each Step Before Absorption**:\n",
        "   $$\n",
        "   E^\\sigma_i \\left[ |r_{X_n}(A_n)| \\right] \\leq r \\cdot \\gamma^n M(i)\n",
        "   $$\n",
        "   - **Interpretation**: Here, $E^\\sigma_i\\left[ |r_{X_n}(A_n)| \\right]$ represents the expected absolute reward at step $n$ while following policy $\\sigma$.\n",
        "   - The bound shows that the expected reward decays exponentially by $\\gamma^n$, and it is also scaled by $M(i)$ and a constant $r$ (related to the maximum possible reward per unit of importance).\n",
        "\n",
        "4. **Bound on Total Expected Reward Until Absorption**:\n",
        "   $$\n",
        "   E^\\sigma_i \\left[ \\sum_{n=0}^{\\tau_\\Delta} |r_{X_n}(A_n)| \\right] \\leq \\frac{r}{1 - \\gamma} M(i)\n",
        "   $$\n",
        "   - **Interpretation**: This part provides a bound on the total expected reward accumulated from the starting state $i$ until the process reaches the absorbing state $\\Delta$.\n",
        "   - It tells us that the total expected reward is finite and depends on the importance weight $M(i)$ and constants $r$ and $\\gamma$. Since $\\gamma < 1$, this total is also bounded, implying that rewards don’t accumulate indefinitely.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary of Lemma 2.1.1\n",
        "\n",
        "This lemma states that, under certain conditions:\n",
        "\n",
        "- The probability of not reaching the absorbing state within $n$ steps decreases exponentially.\n",
        "\n",
        "- The expected time to reach the absorbing state is finite and bounded by $\\frac{M(i)}{1 - \\gamma}$.\n",
        "\n",
        "- The expected reward at each step and the total reward until absorption are both bounded, ensuring that the process does not accumulate excessive rewards over time.\n",
        "\n",
        "Each result in the lemma essentially uses the fact that $\\gamma < 1$ to show that the system’s behavior is controlled and converges towards the absorbing state, rather than diverging.\n",
        "\n",
        "\n",
        "### Algorithm 1: Policy Iteration\n",
        "\n",
        "**Initialization**:  \n",
        "Set $n := 0$. Choose any initial stationary, deterministic policy $f_0 = (f_0, \\dots)$.\n",
        "\n",
        "**Step 1**:  \n",
        "Compute $V_{f_n}$ by solving:\n",
        "$$\n",
        "V_{f_n} = T_{f_n} V_{f_n} = r(f_n) + P(f_n) V_{f_n}.\n",
        "$$\n",
        "For small problems, this can be done by matrix inversion:\n",
        "$$\n",
        "V_{f_n} = (I - P(f_n))^{-1} r(f_n).\n",
        "$$\n",
        "\n",
        "**Step 2**:  \n",
        "Set $f := f_n$ and compute $f_{n+1} = f'$ based on Equation (2.4.1), taking $f = f'$ if possible.\n",
        "\n",
        "**Step 3**:  \n",
        "If $f_{n+1} = f_n$, then this policy is optimal. Stop.  \n",
        "Otherwise, set $n := n + 1$ and return to Step 1.\n"
      ],
      "id": "384a7ca5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define states and actions\n",
        "states = [0, 1]  # For simplicity, consider two states\n",
        "actions = [0, 1]  # Two actions per state\n",
        "\n",
        "# Transition probabilities P[state][action][next_state]\n",
        "# For example, P[0][0][1] = 0.3 means from state 0, taking action 0, there is a 30% chance to move to state 1.\n",
        "P = {\n",
        "    0: {0: [0.7, 0.3], 1: [0.4, 0.6]},  # State 0 transitions\n",
        "    1: {0: [0.2, 0.8], 1: [0.1, 0.9]},  # State 1 transitions\n",
        "}\n",
        "\n",
        "# Rewards for each action in each state\n",
        "# R[state][action] gives the reward for taking the specified action in the given state.\n",
        "R = {\n",
        "    0: {0: 5, 1: 2},  # State 0 rewards\n",
        "    1: {0: 3, 1: 4},  # State 1 rewards\n",
        "}\n",
        "\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.9\n",
        "\n",
        "# Initialize an arbitrary policy where both states take action 0\n",
        "policy = {0: 0, 1: 0}  # Start with action 0 for both states\n",
        "\n",
        "def policy_evaluation(policy):\n",
        "    \"\"\"Evaluate the current policy by solving the linear system for V.\"\"\"\n",
        "    num_states = len(states)\n",
        "    A = np.eye(num_states)  # Identity matrix for (I - gamma * P(policy))\n",
        "    b = np.zeros(num_states)  # Initialize reward vector\n",
        "\n",
        "    # Build system of equations to solve for V\n",
        "    for s in states:\n",
        "        action = policy[s]\n",
        "        # Update the diagonal element for the current state\n",
        "        A[s][s] -= gamma * P[s][action][s]\n",
        "        for next_state in states:\n",
        "            # Update off-diagonal elements for transitions to other states\n",
        "            if next_state != s:\n",
        "                A[s][next_state] -= gamma * P[s][action][next_state]\n",
        "        b[s] = R[s][action]  # Set the reward for current state and action\n",
        "\n",
        "    V = np.linalg.solve(A, b)  # Solve for V using matrix inversion\n",
        "    return V\n",
        "\n",
        "def policy_improvement(V):\n",
        "    \"\"\"Generate a new policy by choosing actions that maximize expected future rewards.\"\"\"\n",
        "    new_policy = {}\n",
        "    for s in states:\n",
        "        # Calculate expected rewards for each action\n",
        "        action_values = [\n",
        "            R[s][a] + gamma * sum(P[s][a][next_state] * V[next_state] for next_state in states)\n",
        "            for a in actions\n",
        "        ]\n",
        "        # Select the action with the maximum expected value\n",
        "        new_policy[s] = np.argmax(action_values)\n",
        "    return new_policy\n",
        "\n",
        "# Main loop for policy iteration\n",
        "is_policy_stable = False\n",
        "iteration = 0\n",
        "while not is_policy_stable:\n",
        "    print(f\"Iteration {iteration}: Policy {policy}\")\n",
        "    # Step 1: Policy Evaluation - calculate value function for current policy\n",
        "    V = policy_evaluation(policy)\n",
        "    # Step 2: Policy Improvement - get a new policy based on the value function\n",
        "    new_policy = policy_improvement(V)\n",
        "    \n",
        "    # Check if the policy has stabilized (no change in policy)\n",
        "    if new_policy == policy:\n",
        "        is_policy_stable = True\n",
        "        print(\"Optimal policy found!\")\n",
        "    else:\n",
        "        policy = new_policy  # Update policy for the next iteration\n",
        "        iteration += 1\n",
        "\n",
        "# Output the final optimal policy and corresponding value function\n",
        "print(\"Final optimal policy:\", policy)\n",
        "print(\"Value function for optimal policy:\", V)"
      ],
      "id": "9196ba29",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define states and actions\n",
        "states = [s for s in range(8)]  # For simplicity, consider two states\n",
        "actions = [0, 1]  # Two actions per state\n",
        "\n",
        "# Transition probabilities P[state][action][next_state]\n",
        "# For example, P[0][0][1] = 0.3 means from state 0, taking action 0, there is a 30% chance to move to state 1.\n",
        "P = {\n",
        "    0: {0: [0.05, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.05], 1: [0.05, 0.1, 0.1, 0.2, 0.2, 0.15, 0.15, 0.05]},  # State 0 transitions\n",
        "    1: {0: [0.05, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.05], 1: [0.05, 0.1, 0.1, 0.2, 0.2, 0.15, 0.15, 0.05]},  # Etc.\n",
        "    2: {0: [0.05, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.05], 1: [0.05, 0.1, 0.1, 0.2, 0.2, 0.15, 0.15, 0.05]},\n",
        "    3: {0: [0.05, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.05], 1: [0.05, 0.1, 0.1, 0.2, 0.2, 0.15, 0.15, 0.05]},\n",
        "    4: {0: [0.05, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.05], 1: [0.05, 0.1, 0.1, 0.2, 0.2, 0.15, 0.15, 0.05]},\n",
        "    5: {0: [0.05, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.05], 1: [0.05, 0.1, 0.1, 0.2, 0.2, 0.15, 0.15, 0.05]},\n",
        "    6: {0: [0.05, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.05], 1: [0.05, 0.1, 0.1, 0.2, 0.2, 0.15, 0.15, 0.05]},\n",
        "    7: {0: [0.05, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.05], 1: [0.05, 0.1, 0.1, 0.2, 0.2, 0.15, 0.15, 0.05]},\n",
        "}\n",
        "\n",
        "# Rewards for each action in each state\n",
        "# R[state][action] gives the reward for taking the specified action in the given state.\n",
        "R = {\n",
        "    0: {0: 5, 1: 2},  # State 0 rewards\n",
        "    1: {0: 3, 1: 4},  # Etc.\n",
        "    2: {0: 3, 1: 4},\n",
        "    3: {0: 3, 1: 4},\n",
        "    4: {0: 3, 1: 4},\n",
        "    5: {0: 3, 1: 4},\n",
        "    6: {0: 3, 1: 4},\n",
        "    7: {0: 3, 1: 4},\n",
        "}\n",
        "\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.9\n",
        "\n",
        "# Initialize an arbitrary policy where both states take action 0\n",
        "policy = {i: 0 for i in range(8)}  # Start with action 0 for both states\n",
        "\n",
        "def policy_evaluation(policy):\n",
        "    \"\"\"Evaluate the current policy by solving the linear system for V.\"\"\"\n",
        "    num_states = len(states)\n",
        "    A = np.eye(num_states)  # Identity matrix for (I - gamma * P(policy))\n",
        "    b = np.zeros(num_states)  # Initialize reward vector\n",
        "\n",
        "    # Build system of equations to solve for V\n",
        "    for s in states:\n",
        "        action = policy[s]\n",
        "        # Update the diagonal element for the current state\n",
        "        A[s][s] -= gamma * P[s][action][s]\n",
        "        for next_state in states:\n",
        "            # Update off-diagonal elements for transitions to other states\n",
        "            if next_state != s:\n",
        "                A[s][next_state] -= gamma * P[s][action][next_state]\n",
        "        b[s] = R[s][action]  # Set the reward for current state and action\n",
        "\n",
        "    V = np.linalg.solve(A, b)  # Solve for V using matrix inversion\n",
        "    return V\n",
        "\n",
        "def policy_improvement(V):\n",
        "    \"\"\"Generate a new policy by choosing actions that maximize expected future rewards.\"\"\"\n",
        "    new_policy = {}\n",
        "    for s in states:\n",
        "        # Calculate expected rewards for each action\n",
        "        action_values = [\n",
        "            R[s][a] + gamma * sum(P[s][a][next_state] * V[next_state] for next_state in states)\n",
        "            for a in actions\n",
        "        ]\n",
        "        # Select the action with the maximum expected value\n",
        "        new_policy[s] = np.argmax(action_values)\n",
        "    return new_policy\n",
        "\n",
        "# Main loop for policy iteration\n",
        "is_policy_stable = False\n",
        "iteration = 0\n",
        "while not is_policy_stable:\n",
        "    print(f\"Iteration {iteration}: Policy {policy}\")\n",
        "    # Step 1: Policy Evaluation - calculate value function for current policy\n",
        "    V = policy_evaluation(policy)\n",
        "    # Step 2: Policy Improvement - get a new policy based on the value function\n",
        "    new_policy = policy_improvement(V)\n",
        "    \n",
        "    # Check if the policy has stabilized (no change in policy)\n",
        "    if new_policy == policy:\n",
        "        is_policy_stable = True\n",
        "        print(\"Optimal policy found!\")\n",
        "    else:\n",
        "        policy = new_policy  # Update policy for the next iteration\n",
        "        iteration += 1\n",
        "\n",
        "# Output the final optimal policy and corresponding value function\n",
        "print(\"Final optimal policy:\", policy)\n",
        "print(\"Value function for optimal policy:\", V)"
      ],
      "id": "2e8574b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from scipy.stats import poisson\n",
        "\n",
        "def f_a(k, l):\n",
        "    return poisson.pmf(k, mu=l)\n",
        "\n",
        "def f_a_larger(k, l):\n",
        "    return 1 - sum(f_a(i, l) for i in range(k + 1))\n",
        "\n",
        "def f_b(b, n):\n",
        "    service_rate = {0: [0.8, 0.2], 1: [0.5, 0.5], 2: [0.2, 0.8]}\n",
        "    return service_rate[b][n - 1]\n",
        "\n",
        "R = 5\n",
        "K = 3\n",
        "\n",
        "def d(b):\n",
        "    service_cost = {0: 0, 1: 2, 2: 5}\n",
        "    return service_cost[b]\n",
        "\n",
        "def k(b, b_prime):\n",
        "    return K if b != b_prime else 0\n",
        "\n",
        "def h(s):\n",
        "    return 2 * s\n",
        "\n",
        "# Reward function\n",
        "def r(s, b, b_prime):\n",
        "    if s == 0:\n",
        "        return - d(b_prime)\n",
        "    elif s == 1:\n",
        "        return R - h(s) - d(b_prime) - k(b, b_prime)\n",
        "    else:\n",
        "        return R * sum(f_b(b_prime, n) * n for n in [1, 2]) - h(s) - d(b_prime) - k(b, b_prime)\n",
        "\n",
        "# Define states and actions\n",
        "states = list(range(8))\n",
        "actions = [0, 1, 2]\n",
        "\n",
        "# Transition probability function\n",
        "def p(s, b, s_prime):\n",
        "    if s <= 1:\n",
        "        return f_a(s_prime, 1.5)\n",
        "    elif s <= 8:\n",
        "        if s_prime < 8:\n",
        "            return sum(f_b(b, n) * f_a(s_prime - s + n, 1.5) for n in range(1, 3))\n",
        "        elif s_prime == 8:\n",
        "            return sum(f_b(b, n) * f_a_larger(s - n - 1, 1.5) for n in range(1, 3))\n",
        "    return 0.0\n",
        "\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.9\n",
        "\n",
        "def policy_evaluation(policy):\n",
        "    \"\"\"Evaluate the current policy by solving the linear system for V.\"\"\"\n",
        "    num_states = len(states)\n",
        "    A = np.eye(num_states)\n",
        "    w = np.zeros(num_states)\n",
        "\n",
        "    for s in states:\n",
        "        action_prime = policy[s]\n",
        "        action = policy[s - 1] if s > 0 else action_prime\n",
        "        A[s][s] -= gamma * p(s, action_prime, s)\n",
        "        for next_state in states:\n",
        "            if next_state != s:\n",
        "                A[s][next_state] -= gamma * p(s, action, next_state)\n",
        "        w[s] = r(s, action, action_prime)\n",
        "\n",
        "    V = np.linalg.solve(A, w)\n",
        "    return V\n",
        "\n",
        "def policy_improvement(V, policy):\n",
        "    \"\"\"Generate a new policy by choosing actions that maximize expected future rewards.\"\"\"\n",
        "    new_policy = {}\n",
        "    for s in states:\n",
        "        action_prime = policy[s]\n",
        "        action_values = [\n",
        "            r(s, b, action_prime) + gamma * sum(p(s, b, next_state) * V[next_state] for next_state in states)\n",
        "            for b in actions\n",
        "        ]\n",
        "        new_policy[s] = np.argmax(action_values)\n",
        "    return new_policy\n",
        "\n",
        "def optimize_policy():\n",
        "    \"\"\"Iteratively improve the policy until it converges to the optimal policy.\"\"\"\n",
        "    policy = {i: 0 for i in range(8)}  # Initialize an arbitrary policy with action 0 for all states\n",
        "    is_policy_stable = False\n",
        "    iteration = 0\n",
        "\n",
        "    while not is_policy_stable:\n",
        "        print(f\"Iteration {iteration}: Policy {policy}\")\n",
        "        \n",
        "        # Step 1: Policy Evaluation - calculate value function for current policy\n",
        "        V = policy_evaluation(policy)\n",
        "        \n",
        "        # Step 2: Policy Improvement - get a new policy based on the value function\n",
        "        new_policy = policy_improvement(V, policy)\n",
        "        \n",
        "        # Check if the policy has stabilized (no change in policy)\n",
        "        if new_policy == policy:\n",
        "            is_policy_stable = True\n",
        "            print(\"Optimal policy found!\")\n",
        "        else:\n",
        "            policy = new_policy  # Update policy for the next iteration\n",
        "            iteration += 1\n",
        "\n",
        "    print(\"Final optimal policy:\", policy)\n",
        "    print(\"Value function for optimal policy:\", V)\n",
        "\n",
        "# Run the policy optimization\n",
        "optimize_policy()"
      ],
      "id": "57eca1cd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}