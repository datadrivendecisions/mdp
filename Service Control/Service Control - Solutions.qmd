---
title: Service Control - Discounted Reward
format:
  html:
    include-in-header:
      - text: |
          <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.4.0/p5.js"></script>
jupyter: python3
keep-ipynb: true
---

## Assignment 2

### Service Rate Control

Consider a discrete-time single-server queueing system that is observed every $\eta > 0$ units of time. The controller makes decisions at times $0, \eta, 2\eta, \dots$. Jobs arrive following a Poisson distribution with rate 1.5 jobs per period of length $\eta$. There is a finite system capacity of eight units; that is, if arriving jobs cause the system content to exceed eight units, excess jobs do not enter the system and are lost.

At each decision epoch, the controller observes the number of jobs in the system and selects the service rate from a set of probability distributions indexed by elements of the set $B = \{0, 1, 2\}$. For each $b \in B$, let $f_b(n)$ denote the probability of $n$ service completions within a period of length $\eta$ with:

-   $f_0(1) = 0.8$, $f_0(2) = 0.2$
-   $f_1(1) = 0.5$, $f_1(2) = 0.5$
-   $f_2(1) = 0.2$, $f_2(2) = 0.8$

The stationary reward structure consists of four components:

1.  A constant reward $R = 5$ for every completed service.
2.  An expected holding cost $h(s) = 2s$ per period when there are $s$ jobs in the system.
3.  A fixed cost $K = 3$ for changing the service rate.
4.  A per-period cost $d(b)$ for using service rate $b$, where $d(0) = 0$, $d(1) = 2$, and $d(2) = 5$.

Determine a minimum-cost service rate control policy.

#### (a)

-   Formulate the problem above as an infinite horizon Markov decision problem.
-   Choose the optimality criterion that you find most reasonable (average costs or discounted costs). Also, choose a method (or methods) for computing the optimal policies and the value. Motivate your choices.
-   Develop the model and the algorithm. Compute the optimal policies and the value. *(Note: you should write your own code for the algorithm, i.e., do not use an existing MDP implementation that is available as a code library or on the internet).*

Please Report: - Model description - Your choice for an optimality criterion including motivation - Solution algorithm (including motivation) - Numerical results and a discussion of those

#### (b)

Now, we require that the server may work at service rate $b = 2$ at most 25% of the time. Model and solve this adjusted problem.

------------------------------------------------------------------------

## Solution

This model is also described in Puterman's book (section 3.7.2) and we follow the same approach but will motivate the modeling choices.

The transition from state $s$ to $s'$ of the queuing system is as follows:

-   After state $s$ the controller chooses a service rate $b'$.
-   After 0, 1, or 2 jobs have departed the system,
-   New jobs arrive.
-   The current number of jobs in the system is state $s'$.

:::: {#fig-status}
::: {#sketch-holder}
:::

System state update between observations $t \text{ and } t+1$
::::

<script>
    function setup() {
        const canvas = createCanvas(800, 200);
        canvas.parent('sketch-holder');
        textAlign(CENTER, CENTER);
        textSize(14);
    }

    function draw() {
        background(255);
        
        // Timeline base
        const startX = 50;
        const endX = 750;
        const baseY = 100;
        strokeWeight(2);
        line(startX, baseY, endX, baseY);
        
        // Draw events
        const events = [
            { x: 150, label: "State s", symbol: "(x,b)" },
            { x: 300, label: "Choose\nservice rate", symbol: "b'" },
            { x: 450, label: "Jobs depart\n(0,1,2)", symbol: "↓" },
            { x: 600, label: "New jobs\narrive", symbol: "↑" },
            { x: 700, label: "State s'", symbol: "(x',b')" }
        ];
        
        // Draw arrows and events
        strokeWeight(1);
        for (let i = 0; i < events.length; i++) {
            const event = events[i];
            
            // Draw vertical line
            line(event.x, baseY - 20, event.x, baseY + 20);
            
            // Draw label
            fill(0);
            noStroke();
            text(event.label, event.x, baseY + 45);
            
            // Draw state/symbol
            textSize(16);
            text(event.symbol, event.x, baseY - 35);
            textSize(14);
            
            // Draw arrow to next event if not last event
            if (i < events.length - 1) {
                stroke(0);
                drawArrow(event.x, baseY, events[i + 1].x - 40, baseY);
            }
        }
    }
    
    function drawArrow(x1, y1, x2, y2) {
        line(x1, y1, x2, y2);
        push();
        translate(x2, y2);
        const angle = atan2(y2 - y1, x2 - x1);
        rotate(angle);
        const arrowSize = 8;
        triangle(0, arrowSize/2, 0, -arrowSize/2, arrowSize, 0);
        pop();
    }
</script>

To formulate this problem as an infinite horizon Markov decision process (MDP), we define the key components:

#### Decision epochs ($T$)

$T = \{0, 1, 2, \dots\}$, corresponding to observation times $0, \eta, 2\eta, \dots$

#### State space ($S$)

The state $s = (x, b)$ consists of the number of jobs $x \in X$ in the system: $X = \{0, 1, 2, \dots, 8\}$

and the current service rate $b \in B$: $B = \{0, 1, 2\}$.

#### Action space ($B$)

At each decision epoch, the controller can take an action and selects a service rate $b' \in B$ for the next epoch.

#### Transition probabilities ($P$)

The transition probabilities depend on the arrival rate (Poisson distribution with a rate of 1.5 jobs per period of length $\eta$) and the service rate $b'$ chosen.

-   Let $f^a(k)$ denote the probability of $k$ job arrivals in a period of length $\eta$. Since arrivals follow a Poisson distribution with rate 1.5:

$$
f_a(k)=\begin{cases}
\frac{(1.5)^k e^{-1.5}}{k!}, & \text{if } k = 0, 1, 2, \dots \\
0, & \text{otherwise}
\end{cases}
$$

-   Let $f_b(n)$ denote the probability of $n \in \{1, 2\}$ service completions under service rate $b$, where the distributions of $n$ are given for each $b \in B$. This only applies to starting states $s$ with $x > 1$ when the system contains enough jobs to be processed.

Define $p(s' | s, b')$ as the probability of transitioning to state $s' = (x',b')$ when the system is in state $s = (x, b)$ and service state $b'$ is chosen. We can then distinguish the following cases:

$$
p(s'|s, b') = 
\begin{cases} 
f^a(x') & \text{if } x \in \{0, 1\} \text{ and } x' < 8, \\
1 - \sum_{k=0}^{7} f^a(k) & \text{if } x \in \{0, 1\} \text{ and } x' = 8, \\
\sum_{n=1}^{2} f_{b'}(n) \cdot f^a(x' - x + n) & \text{if } 1 < x \leq 8 \text{ and } x' < 8, \\
\sum_{n=1}^{2} f_{b'}(n) \cdot \left(1 - \sum_{k=0}^{7 - x + n} f^a(k)\right) & \text{if } 1 < x \leq 8 \text{ and } x' = 8.
\end{cases}
$$

#### Cost and rewards ($C$)

The cost function consists of four components:

1.  **Reward for service completions**: For each service completion $n$, the system receives a reward of $R = 5$. The total reward for service completions when the system is in state $s$ and action $b'$ is chosen is: $5n$.

2.  **Holding cost**: The expected holding cost per period is $h(x) = 2x$ when there are $x$ jobs in the system.

3.  **Cost of changing service rate**: If the controller changes the service rate from one period to the next ($b \neq b'$), a fixed cost of $K = 3$ is incurred.

4.  **Per-period cost for using service rate**: The cost $d(b')$ depends on the chosen service rate $b'$, where: $d(0) = 0, \quad d(1) = 2, \quad d(2) = 5$

The total cost function can thus be written as:
$$
C_n(s, b') = h(x) + d(b') + K \cdot \mathbf{1}_{\{b \neq b'\}} - Rn \\
$$

#### Solution

The equilibrium probabilities under stationary policy $f$, $\pi^f_{s'}$ are the unique solution to eqquilibrium equations: 

$$
\pi^f_j = \sum_{i \in S} \pi^f_i \cdot p(j|i, f(i)),
$$
$$
\sum_{j \in S} \pi^f_{j} = 1.
$$
The long-run average cost per unit time under a stationary policy f is given by:

$$
g^f = \sum_{j \in S} \pi^f_j \cdot C(j, f(j)).
$$
A stationary policy $f^*$ is said to be **average cost optimal** if

$$
g^{f^*} \leq g^{f} \quad \text{for each stationary policy } f.
$$
---

### Code

#### Setup

**Parameters:**

- **State Space ($S$)**: Each state is a tuple $(s, b)$, where:
  - $s$ is the number of jobs in the system (ranging from 0 to $S_{\text{max}} = 8$).
  - $b$ is the current service rate (from the set $B = \{0, 1, 2\}$).

- **Action Space ($A$)**: The actions are the possible service rates $b' \in B$ that the controller can choose at each decision epoch.

- **Transition Probabilities ($P$)**: The function `P(s_i, b_i, s_j, b_j, a)` calculates the probability of moving from state $(s_i, b_i)$ to $(s_j, b_j)$ given action $a$. It accounts for:
  - The probability of job arrivals (modeled using a Poisson distribution with rate 1.5).
  - The probability of job completions based on the chosen service rate's distribution $f_b(n)$.

- **Reward Function ($r(s, b, a)$)**: Computes the immediate reward for being in state $(s, b)$ and choosing action $a$. It includes:
  - **Processing Rewards**: Earned for completing jobs.
  - **Holding Costs**: Penalty for the number of jobs in the system.
  - **Operating Costs**: Cost of using a specific service rate.
  - **Switching Costs**: Additional cost if the service rate changes (i.e., $b \neq a$).

```{python}
##Initialize packages, parameters and functions

from ortools.linear_solver import pywraplp
from scipy.stats import poisson

# Parameters
d = { # cost for using server rate
        0: 0,
        1: 2,
        2: 5
    }  
K = 3  # cost for changing the service rate
R = 5  # reward for processing 1 job per period
H = 2 # holding cost per period
S_max = 8  # max system capacity
l = 0.95 # discount factor lambda

# cost function for calculating the holding cost
def h(s, H):
    return H * s # expected holding cost per period

# probability f_b(n) of processing n jobs if the service rate is b
def f(n, b):
    b_prob = {0: [0.8, 0.2], 1: [0.5, 0.5], 2: [0.2, 0.8]}
    return b_prob[b][n - 1]

# probability g(n) of n new jobs arriving
def g(n):
    return poisson.pmf(n, mu=1.5)

# probability of n or more new jobs arriving
def g_or_more(n):
    return 1 - sum(g(m) for m in range(n))

# Sets of states (S: number of jobs in the system, B: server states)
S = range(S_max + 1)
B = [0, 1, 2]

# Define the expected reward R_{(s,b),a'} when state is (s,b) and chosen service rate is a'
def r(s, b, a):
    if s == 0:
        return - d[a] - K*(b != a) # no costs for processing and holding jobs
    elif s == 1:
        return R - h(1, H) - d[a] - K*(b != a) # reward and cost for processing and holding 1 job
    else:
        return R * sum(f(n, a) * n for n in [1, 2]) - h(s, H) - d[a] - K*(b != a)

# Transition probabilities
def P(s_i, b_i, s_j, b_j, a):
    if b_j != a:
        return 0.0
    if s_i <= 1:
        if s_j < S_max:
            return g(s_j) # Arrival rate
        elif s_j == S_max:
            return g_or_more(s_j) # 1 - probability of 0, 1, 2, 3, 4, 5, 6, 7 arrivals
        else:
            return 0.0 # s_j > S_max, not possible
    elif s_i <= S_max:
        if s_j < S_max:
            return sum(f(n, b_j) * g(s_j - s_i + n) for n in [1, 2]) # Arrivals have to compensate for the difference in states and processed jobs
        elif s_j == S_max:
            return sum(f(n, b_j) * g_or_more(s_j - s_i + n) for n in [1, 2]) # Arrivals have to compensate for the difference in states and processed jobs
        else:
            return 0.0 # s_j > S_max, not possible
    else:
        return 0.0 # s_i > S_max, not possible
```


#### Optimality criterion: discounted reward

In this solution, we model the problem as a Markov Decision Process (MDP) with the goal of **maximizing the expected discounted reward** over an infinite horizon. The discount factor $\lambda = 0.95$ is applied to future rewards, reflecting the preference for immediate rewards over distant ones.

**Code Explanation:**

- **Initialization**:
  - Import necessary libraries (`pywraplp` for the solver and `poisson` from `scipy.stats`).
  - Define parameters such as costs, rewards, and probabilities.
  - Define helper functions for holding costs (`h`), processing probabilities (`f`), and arrival probabilities (`g` and `g_or_more`).

- **Decision Variables**:
  - Create variables $x_{(s,b),a}$ representing the probability of being in state $(s, b)$ and choosing action $a$.
  - These variables are created using `solver.NumVar`.

- **Objective Function**:
  - Constructed to maximize the expected discounted reward.
  - Uses the immediate reward function `r(s, b, a)` and sums over all states and actions.
  - Set up using `solver.Objective` and `objective.SetCoefficient`.

- **Balance Constraints**:
  - Ensure the flow of probabilities between states adheres to the MDP dynamics.
  - Incorporate the discount factor $\lambda$.
  - Added using `solver.Add`, ensuring that the probability of being in a state equals the discounted sum of probabilities of transitioning into that state.

- **Solving and Output**:
  - The solver is executed with `solver.Solve()`.
  - Upon finding an optimal solution, the code prints the optimal actions for each state where the decision variable $x$ is greater than zero.

```{python}
# Create the solver
solver = pywraplp.Solver.CreateSolver('GLOP')

# Decision variables x_{(s,b),b'}: probability of choosing server state b' when state is (s,b)
x = {}
for s in S:
    for b in B:
        for b_prime in B:
            x[(s, b, b_prime)] = solver.NumVar(0.0, 1.0, f'x_{s}_{b}_{b_prime}')

# Initial state probabilities alpha
alpha = {}
n_total_states = len(S) * len(B)
for s in S:
    for b in B:
        alpha[(s, b)] = 1 / n_total_states
        
# Objective: maximize the expected reward    
objective = solver.Objective()
for s in S:
    for b in B:
        for a in B:
            objective.SetCoefficient(x[(s, b, a)], r(s, b, a))
objective.SetMaximization()

# Balance constraints
for s_j in S:
    for b_j in B:
        lhs = solver.Sum(x[(s_j, b_j, a)] for a in B)
        rhs = l * solver.Sum(
          P(s_i, b_i, s_j, b_j, a) * x[(s_i, b_i, a)] 
          for s_i in S for b_i in B for a in B
          )
        solver.Add(lhs == alpha[(s_j, b_j)] + rhs)

# Solve the problem
status = solver.Solve()

# Print the optimal solution
if status == pywraplp.Solver.OPTIMAL:
    for s in S:
        print(f"If {s} jobs in the system:")
        for b in B:
            print(f"   If previous service rate was {b}:")
            for a in B:
                x_opt = x[(s, b, a)].solution_value()
                if x_opt > 0:
                    print(f"      Use service rate {a} (x={x_opt})")
else:
    print("No feasible solution found.")
```

#### Optimality criterion: average reward

In this solution, the objective shifts to **maximizing the average reward per period** without considering discounting. This reflects a long-term perspective where future rewards are valued equally with immediate rewards.

**Solution Design:**

- **Balance Constraints**:
  - Modified to ensure that the steady-state probabilities satisfy the flow conservation equations.
  - The inflow to each state equals the outflow, reflecting a stable long-term system.

- **Total Probability Constraint**:
  - An additional constraint ensures that the sum of all probabilities $x_{(s,b),a}$ equals 1.
  - This ensures that the decision variables represent a valid probability distribution.
  
**Code Explanation:**

- **Initialization**: Similar to the previous model.

- **Decision Variables and Objective Function**:
  - Variables $x_{(s,b),a}$ are defined in the same way.
  - The objective function maximizes the average reward, summing over all states and actions without discounting.

- **Balance Constraints**:
  - The constraints now equate the total inflow and outflow probabilities for each state $(s_j, b_j)$.
  - Implemented using `solver.Add`, ensuring that for each state, the sum of incoming probabilities equals the sum of outgoing probabilities.

- **Total Probability Constraint**:
  - Ensures the sum of all $x_{(s,b),a}$ is 1.
  - Added using `solver.Add`.

- **Solving and Output**:
  - The solver is executed, and upon finding an optimal solution, the code prints the optimal actions for each state.

```{python}
# Create the solver
solver = pywraplp.Solver.CreateSolver('GLOP')
solver.SetTimeLimit(60000)  # Set time limit in milliseconds (60 seconds)

# Decision variables x_{(s,b),b'}: probability of choosing server state b' when state is (s,b)
x = {}
for s in S:
    for b in B:
        for b_prime in B:
            x[(s, b, b_prime)] = solver.NumVar(0.0, 1.0, f'x_{s}_{b}_{b_prime}')

# Initial state probabilities alpha
alpha = {}
n_total_states = len(S) * len(B)
for s in S:
    for b in B:
        alpha[(s, b)] = 1 / n_total_states

# Objective: maximize the expected reward
objective = solver.Objective()
for s in S:
    for b in B:
        for a in B:
            objective.SetCoefficient(x[(s, b, a)], r(s, b, a))
objective.SetMaximization()

# Balance constraints
for s_j in S:
    for b_j in B:
        # Right-hand side (outflow) - the sum of probabilities of choosing each action a in state (s_j, b_j)
        outflow = solver.Sum(x[(s_j, b_j, a)] for a in B)
        
        # Left-hand side (inflow) - the weighted sum of transition probabilities for each incoming state-action pair
        inflow = solver.Sum(
            P(s_i, b_i, s_j, b_j, a) * x[(s_i, b_i, a)]
            for s_i in S for b_i in B for a in B
        )
        
        # Add the constraint that the inflow and outflow must be equal
        solver.Add(inflow == outflow)

# Probability constraint to ensure all probabilities sum to 1
solver.Add(solver.Sum(x[(s, b, a)] for s in S for b in B for a in B) == 1)

# Solve the problem
status = solver.Solve()

# Print the optimal solution
if status == pywraplp.Solver.OPTIMAL:
    for s in S:
        print(f"If {s} jobs in the system:")
        for b in B:
            print(f"   If previous service rate was {b}:")
            for a in B:
                x_opt = x[(s, b, a)].solution_value()
                if x_opt > 0:
                    print(f"      Use service rate {a} (x={x_opt:.2f})")
else:
    print("No feasible solution found.")
```

#### Optimality criterion: discounted reward with service rate restriction

This solution introduces a **service rate restriction** into the discounted reward model, limiting the usage of the highest service rate $b = 2$ to at most 25% of the time. This constraint could represent resource limitations or cost considerations.

**Solution Design:**

- **Service Rate Restriction**:
  - A new constraint is added to limit the total probability of choosing service rate $a = 2$.
  - The constraint ensures that the sum of probabilities where $a = 2$ does not exceed 25% of the total probability.

**Code Explanation:**

- **Initialization, Decision Variables, and Objective Function**: Same as in the discounted reward model.

- **Balance Constraints**: Remain the same.

- **Service Rate Restriction Constraint**:
  - Calculated the total probability of choosing service rate $a = 2$ across all states.

```{python}
# Create the solver
solver = pywraplp.Solver.CreateSolver('GLOP')

# Decision variables x_{(s,b),b'}: probability of choosing server state b' when state is (s,b)
x = {}
for s in S:
    for b in B:
        for b_prime in B:
            x[(s, b, b_prime)] = solver.NumVar(0.0, 1.0, f'x_{s}_{b}_{b_prime}')

# Initial state probabilities alpha
alpha = {}
n_total_states = len(S) * len(B)
for s in S:
    for b in B:
        alpha[(s, b)] = 1 / n_total_states
        
# Objective: maximize the expected reward    
objective = solver.Objective()
for s in S:
    for b in B:
        for a in B:
            objective.SetCoefficient(x[(s, b, a)], r(s, b, a))
objective.SetMaximization()

# Balance constraints
for s_j in S:
    for b_j in B:
        lhs = solver.Sum(x[(s_j, b_j, a)] for a in B)
        rhs = l * solver.Sum(
          P(s_i, b_i, s_j, b_j, a) * x[(s_i, b_i, a)] 
          for s_i in S for b_i in B for a in B
          )
        solver.Add(lhs == alpha[(s_j, b_j)] + rhs)
        
# Define the left-hand side: the total probability of using server state b = 2
service_rate_b2 = solver.Sum(x[(s, 2, a)] for s in S for a in B)

# Define the right-hand side: 25% of the total probability across all states, server states, and actions
total_probability = solver.Sum(x[(s, b, a)] for s in S for b in B for a in B)
solver.Add(service_rate_b2 <= 0.25 * total_probability)

# Solve the problem
status = solver.Solve()

# Print the optimal solution
if status == pywraplp.Solver.OPTIMAL:
    for s in S:
        print(f"If {s} jobs in the system:")
        for b in B:
            print(f"   If previous service rate was {b}:")
            for a in B:
                x_opt = x[(s, b, a)].solution_value()
                if x_opt > 0:
                    print(f"      Use service rate {a} (x={x_opt})")
else:
    print("No feasible solution found.")
```

#### Optimality criterion: average reward with service rate restriction

This solution applies the **service rate restriction** to the average reward model, limiting the usage of the highest service rate $b = 2$ while aiming to maximize the average reward.

**Solution Design:**

- **State and Action Spaces, Transition Probabilities, and Reward Function**: Same as in the average reward model.

- **Service Rate Restriction**:
  - Similar to the previous solution, a constraint is added to limit the total probability of choosing service rate $a = 2$.

**Code Explanation:**

- **Initialization, Decision Variables, Objective Function, and Balance Constraints**: Same as in the average reward model.

- **Total Probability Constraint**: Ensures the probabilities sum to 1.

```{python}
# Create the solver
solver = pywraplp.Solver.CreateSolver('GLOP')
solver.SetTimeLimit(60000)  # Set time limit in milliseconds (60 seconds)

# Decision variables x_{(s,b),b'}: probability of choosing server state b' when state is (s,b)
x = {}
for s in S:
    for b in B:
        for b_prime in B:
            x[(s, b, b_prime)] = solver.NumVar(0.0, 1.0, f'x_{s}_{b}_{b_prime}')

# Initial state probabilities alpha
alpha = {}
n_total_states = len(S) * len(B)
for s in S:
    for b in B:
        alpha[(s, b)] = 1 / n_total_states

# Objective: maximize the expected reward
objective = solver.Objective()
for s in S:
    for b in B:
        for a in B:
            objective.SetCoefficient(x[(s, b, a)], r(s, b, a))
objective.SetMaximization()

# Balance constraints
for s_j in S:
    for b_j in B:
        # Right-hand side (outflow) - the sum of probabilities of choosing each action a in state (s_j, b_j)
        outflow = solver.Sum(x[(s_j, b_j, a)] for a in B)
        
        # Left-hand side (inflow) - the weighted sum of transition probabilities for each incoming state-action pair
        inflow = solver.Sum(
            P(s_i, b_i, s_j, b_j, a) * x[(s_i, b_i, a)]
            for s_i in S for b_i in B for a in B
        )
        
        # Add the constraint that the inflow and outflow must be equal
        solver.Add(inflow == outflow)

# Probability constraint to ensure all probabilities sum to 1
solver.Add(solver.Sum(x[(s, b, a)] for s in S for b in B for a in B) == 1)

# Service rate restriction: Limit the probability of using server state b = 2 to 25%
service_rate_limit = solver.Sum(x[(s, 2, a)] for s in S for a in B)
solver.Add(service_rate_limit <= 0.25)

# Solve the problem
status = solver.Solve()

# Print the optimal solution
if status == pywraplp.Solver.OPTIMAL:
    for s in S:
        print(f"If {s} jobs in the system:")
        for b in B:
            print(f"   If previous service rate was {b}:")
            for a in B:
                x_opt = x[(s, b, a)].solution_value()
                if x_opt > 0:
                    print(f"      Use service rate {a} (x={x_opt:.2f})")
else:
    print("No feasible solution found.")
```

**General Notes on the Code:**

- **Solver Configuration**:
  - All models use the OR-Tools `GLOP` linear solver.
  - Time limits are set where necessary to prevent long computation times.

- **Printing Results**:
  - The code checks if an optimal solution is found.
  - It iterates over all states and prints the optimal action(s) where the decision variable $x$ is positive, indicating the action(s) to take in each state.