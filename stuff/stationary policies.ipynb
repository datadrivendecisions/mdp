{
 "cells": [
  {
   "cell_type": "raw",
   "id": "60befbc8",
   "metadata": {},
   "source": [
    "---\n",
    "title: Stationary policy equilibrium probabilities\n",
    "format: html\n",
    "editor: visual\n",
    "keep-ipynb: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c644cbe",
   "metadata": {},
   "source": [
    "## ![Slide from MDP course LNMB](images/stationary-policies.png)\n",
    "\n",
    "## **Equilibrium Equations for Markov Decision Processes**\n",
    "\n",
    "Given a set of states $S$, a stationary policy $f$, and transition probabilities $p_{kj}(a)$ where $a = f(k)$ is the action taken in state $k$, the equilibrium (steady-state) probabilities $\\pi^f_j$ satisfy the following system of equations:\n",
    "\n",
    "### **1. Equilibrium Equations**\n",
    "\n",
    "For each state $j \\in S$:\n",
    "\n",
    "$$\n",
    "\\pi^f_j = \\sum_{k \\in S} \\pi^f_k \\cdot p_{kj}(f(k))\n",
    "$$\n",
    "\n",
    "This equation states that the steady-state probability of being in state $j$ is the sum over all states $k$ of the probability of being in state $k$ and moving to state $j$ under policy $f$.\n",
    "\n",
    "### **2. Normalization Constraint**\n",
    "\n",
    "The probabilities must sum to 1:\n",
    "\n",
    "$$\n",
    "\\sum_{j \\in S} \\pi^f_j = 1\n",
    "$$\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Example: Two-State Markov Chain**\n",
    "\n",
    "Let's consider a simple Markov chain with two states $S = \\{0, 1\\}$ and a policy $f$ that always chooses action $a = 0$. The transition probabilities under action $0$ are:\n",
    "\n",
    "-   $p_{00}(0) = 0.7$: Probability of staying in state $0$\n",
    "-   $p_{01}(0) = 0.3$: Probability of moving from state $0$ to $1$\n",
    "-   $p_{10}(0) = 0.4$: Probability of moving from state $1$ to $0$\n",
    "-   $p_{11}(0) = 0.6$: Probability of staying in state $1$\n",
    "\n",
    "### **Equilibrium Equations**\n",
    "\n",
    "We write the equilibrium equations for each state.\n",
    "\n",
    "#### **For State** $j = 0$:\n",
    "\n",
    "$$\n",
    "\\pi^f_0 = \\pi^f_0 \\cdot p_{00}(0) + \\pi^f_1 \\cdot p_{10}(0)\n",
    "$$\n",
    "\n",
    "Substitute the known probabilities:\n",
    "\n",
    "$$\n",
    "\\pi^f_0 = \\pi^f_0 \\cdot 0.7 + \\pi^f_1 \\cdot 0.4\n",
    "$$\n",
    "\n",
    "#### **For State** $j = 1$:\n",
    "\n",
    "$$\n",
    "\\pi^f_1 = \\pi^f_0 \\cdot p_{01}(0) + \\pi^f_1 \\cdot p_{11}(0)\n",
    "$$\n",
    "\n",
    "Substitute the known probabilities:\n",
    "\n",
    "$$\n",
    "\\pi^f_1 = \\pi^f_0 \\cdot 0.3 + \\pi^f_1 \\cdot 0.6\n",
    "$$\n",
    "\n",
    "### **Normalization Constraint**\n",
    "\n",
    "$$\n",
    "\\pi^f_0 + \\pi^f_1 = 1\n",
    "$$\n",
    "\n",
    "### **Solving the Equations**\n",
    "\n",
    "We can rearrange the equations to solve for $\\pi^f_0$ and $\\pi^f_1$.\n",
    "\n",
    "#### **Equation for State** $0$:\n",
    "\n",
    "$$\n",
    "\\pi^f_0 = 0.7 \\pi^f_0 + 0.4 \\pi^f_1\n",
    "$$\n",
    "\n",
    "Subtract $0.7 \\pi^f_0$ from both sides:\n",
    "\n",
    "$$\n",
    "\\pi^f_0 - 0.7 \\pi^f_0 = 0.4 \\pi^f_1\n",
    "$$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$\n",
    "0.3 \\pi^f_0 = 0.4 \\pi^f_1\n",
    "$$\n",
    "\n",
    "#### **Equation for State** $1$:\n",
    "\n",
    "$$\n",
    "\\pi^f_1 = 0.3 \\pi^f_0 + 0.6 \\pi^f_1\n",
    "$$\n",
    "\n",
    "Subtract $0.6 \\pi^f_1$ from both sides:\n",
    "\n",
    "$$\n",
    "\\pi^f_1 - 0.6 \\pi^f_1 = 0.3 \\pi^f_0\n",
    "$$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$\n",
    "0.4 \\pi^f_1 = 0.3 \\pi^f_0\n",
    "$$\n",
    "\n",
    "#### **Set Up the System of Equations**\n",
    "\n",
    "Now we have:\n",
    "\n",
    "1.  $0.3 \\pi^f_0 = 0.4 \\pi^f_1$  (1)\n",
    "2.  $0.4 \\pi^f_1 = 0.3 \\pi^f_0$  (2) (Note: This is the same as equation (1))\n",
    "3.  $\\pi^f_0 + \\pi^f_1 = 1$     (3)\n",
    "\n",
    "Since equations (1) and (2) are the same, we can use one of them along with the normalization constraint.\n",
    "\n",
    "#### **Solve for** $\\pi^f_1$ in Terms of $\\pi^f_0$\n",
    "\n",
    "From equation (1):\n",
    "\n",
    "$$\n",
    "0.3 \\pi^f_0 = 0.4 \\pi^f_1\n",
    "$$\n",
    "\n",
    "Solve for $\\pi^f_1$:\n",
    "\n",
    "$$\n",
    "\\pi^f_1 = \\frac{0.3}{0.4} \\pi^f_0 = \\frac{3}{4} \\pi^f_0\n",
    "$$\n",
    "\n",
    "#### **Use the Normalization Constraint**\n",
    "\n",
    "From equation (3):\n",
    "\n",
    "$$\n",
    "\\pi^f_0 + \\pi^f_1 = 1\n",
    "$$\n",
    "\n",
    "Substitute $\\pi^f_1$:\n",
    "\n",
    "$$\n",
    "\\pi^f_0 + \\frac{3}{4} \\pi^f_0 = 1\n",
    "$$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$\n",
    "\\pi^f_0 \\left(1 + \\frac{3}{4}\\right) = 1 \\\\\n",
    "\\pi^f_0 \\left(\\frac{4}{4} + \\frac{3}{4}\\right) = 1 \\\\\n",
    "\\pi^f_0 \\left(\\frac{7}{4}\\right) = 1\n",
    "$$\n",
    "\n",
    "Solve for $\\pi^f_0$:\n",
    "\n",
    "$$\n",
    "\\pi^f_0 = \\frac{1}{\\frac{7}{4}} = \\frac{4}{7}\n",
    "$$\n",
    "\n",
    "#### **Find** $\\pi^f_1$\n",
    "\n",
    "$$\n",
    "\\pi^f_1 = \\frac{3}{4} \\pi^f_0 = \\frac{3}{4} \\cdot \\frac{4}{7} = \\frac{3}{7}\n",
    "$$\n",
    "\n",
    "### **Final Equilibrium Probabilities**\n",
    "\n",
    "-   $\\pi^f_0 = \\dfrac{4}{7} \\approx 0.5714$\n",
    "-   $\\pi^f_1 = \\dfrac{3}{7} \\approx 0.4286$\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "The system of equilibrium equations consists of:\n",
    "\n",
    "-   **One equation per state**: Each expresses the balance between the probability of being in a state and the probabilities of transitioning into that state from all other states under policy $f$.\n",
    "\n",
    "-   **Normalization constraint**: Ensures that the probabilities sum to 1.\n",
    "\n",
    "These equations can be set up in matrix form and solved using linear algebra techniques.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "**Note**: In larger Markov chains, the system of equations may become more complex, but the principle remains the same. Solving the equilibrium equations provides the steady-state probabilities under a given policy, which is essential for analyzing long-run behavior and performance metrics in Markov Decision Processes.\n",
    "\n",
    "## Code\n",
    "\n",
    "To compute the equilibrium probabilities $\\pi^f_j$ under a stationary policy $f$, you can solve the system of linear equations defined by:\n",
    "\n",
    "1.  **Equilibrium Equations**:\n",
    "\n",
    "    $$\n",
    "    \\pi^f_j = \\sum_{k \\in S} \\pi^f_k \\cdot p_{kj}(f(k)) \\quad \\text{for all } j \\in S\n",
    "    $$\n",
    "\n",
    "2.  **Normalization Constraint**:\n",
    "\n",
    "    $$\n",
    "    \\sum_{j \\in S} \\pi^f_j = 1\n",
    "    $$\n",
    "\n",
    "These equations state that the probability of being in state $j$ is equal to the sum over all states $k$ of the probability of being in state $k$ and moving to state $j$ under policy $f$.\n",
    "\n",
    "### **Implementation in Code**\n",
    "\n",
    "Below is a Python code example that computes the equilibrium probabilities $\\pi^f_j$ given:\n",
    "\n",
    "-   A set of states $S$.\n",
    "-   A stationary policy $f$ that maps each state $k \\in S$ to an action $a \\in A$.\n",
    "-   Transition probabilities $p_{kj}(a)$ for each action $a$.\n",
    "\n",
    "We'll use NumPy for numerical computations.\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "\n",
    "# Define the set of states\n",
    "S = [...]  # List of states, e.g., S = [0, 1, 2, ..., S_max]\n",
    "\n",
    "# Define the action set (if needed)\n",
    "A = [...]  # List of actions, e.g., A = [0, 1, 2]\n",
    "\n",
    "# Define the policy function f(k): returns the action to take in state k\n",
    "def f(k):\n",
    "    # Example policy: always choose action 1\n",
    "    # Replace this with your actual policy\n",
    "    return ...\n",
    "\n",
    "# Define the transition probability function p_kj(k, j, a): returns p_{kj}(a)\n",
    "def p_kj(k, j, a):\n",
    "    # Replace this with your actual transition probabilities\n",
    "    # For example:\n",
    "    # if k == j:\n",
    "    #     return 0.5\n",
    "    # else:\n",
    "    #     return 0.5 / (len(S) - 1)\n",
    "    return ...\n",
    "\n",
    "# Map states to indices for matrix computations\n",
    "state_to_index = {state: idx for idx, state in enumerate(S)}\n",
    "index_to_state = {idx: state for state, idx in state_to_index.items()}\n",
    "\n",
    "# Number of states\n",
    "n_states = len(S)\n",
    "\n",
    "# Construct the transition matrix P under policy f\n",
    "P = np.zeros((n_states, n_states))\n",
    "\n",
    "for k in S:\n",
    "    a = f(k)\n",
    "    k_idx = state_to_index[k]\n",
    "    for j in S:\n",
    "        j_idx = state_to_index[j]\n",
    "        P[k_idx, j_idx] = p_kj(k, j, a)\n",
    "\n",
    "# Transpose P for solving the left eigenvector\n",
    "P_T = P.T\n",
    "\n",
    "# Set up the equations (P_T - I) * pi = 0\n",
    "A = P_T - np.eye(n_states)\n",
    "\n",
    "# Add the normalization constraint sum(pi) = 1\n",
    "A = np.vstack([A, np.ones(n_states)])\n",
    "b = np.zeros(n_states)\n",
    "b = np.append(b, 1)\n",
    "\n",
    "# Solve the linear system\n",
    "# Use least squares in case the system is singular\n",
    "pi, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n",
    "\n",
    "# Display the equilibrium probabilities\n",
    "print(\"Equilibrium probabilities π^f_j under policy f:\")\n",
    "for idx, prob in enumerate(pi):\n",
    "    state = index_to_state[idx]\n",
    "    print(f\"State {state}: π^f_{state} = {prob}\")\n",
    "```\n",
    "\n",
    "### **Explanation of the Code**\n",
    "\n",
    "-   **State and Action Sets**:\n",
    "    -   `S`: List of all possible states in your Markov chain.\n",
    "    -   `A`: List of possible actions (if applicable).\n",
    "-   **Policy Function `f(k)`**:\n",
    "    -   Defines the action to take when in state `k`.\n",
    "    -   Replace the placeholder with your specific policy.\n",
    "-   **Transition Probability Function `p_kj(k, j, a)`**:\n",
    "    -   Returns the probability of transitioning from state `k` to state `j` given action `a`.\n",
    "    -   Replace the placeholder with your actual transition probabilities.\n",
    "-   **Transition Matrix `P`**:\n",
    "    -   A square matrix of size `n_states x n_states`.\n",
    "    -   Each element `P[k_idx, j_idx]` represents the probability of moving from state `k` to state `j` under policy `f`.\n",
    "-   **Solving the Linear System**:\n",
    "    -   We need to solve for the vector $\\pi$ such that $\\pi = \\pi P$.\n",
    "    -   This is equivalent to solving $(P^T - I) \\pi = 0$, where $I$ is the identity matrix.\n",
    "    -   We add the normalization constraint $\\sum_j \\pi_j = 1$ by appending a row of ones to `A` and appending `1` to `b`.\n",
    "-   **Least Squares Solution**:\n",
    "    -   We use `np.linalg.lstsq` to solve the linear system.\n",
    "    -   This method is robust to singular matrices and provides a least-squares solution if an exact solution doesn't exist.\n",
    "\n",
    "### **Example with a Concrete MDP**\n",
    "\n",
    "Suppose we have a simple MDP with states `S = [0, 1]` and actions `A = [0, 1]`. Let's define:\n",
    "\n",
    "-   Policy $f$ that always chooses action `0`.\n",
    "\n",
    "-   Transition probabilities:\n",
    "\n",
    "    -   If action `0` is taken:\n",
    "\n",
    "        -   From state `0` to `0`: `p_00(0) = 0.7`\n",
    "        -   From state `0` to `1`: `p_01(0) = 0.3`\n",
    "        -   From state `1` to `0`: `p_10(0) = 0.4`\n",
    "        -   From state `1` to `1`: `p_11(0) = 0.6`\n",
    "\n",
    "Implementing this example:\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "\n",
    "# Define the set of states\n",
    "S = [0, 1]\n",
    "\n",
    "# Define the policy function f(k): always choose action 0\n",
    "def f(k):\n",
    "    return 0\n",
    "\n",
    "# Define the transition probability function p_kj(k, j, a)\n",
    "def p_kj(k, j, a):\n",
    "    if a == 0:\n",
    "        if k == 0 and j == 0:\n",
    "            return 0.7\n",
    "        elif k == 0 and j == 1:\n",
    "            return 0.3\n",
    "        elif k == 1 and j == 0:\n",
    "            return 0.4\n",
    "        elif k == 1 and j == 1:\n",
    "            return 0.6\n",
    "    return 0.0  # Default case\n",
    "\n",
    "# Map states to indices\n",
    "state_to_index = {state: idx for idx, state in enumerate(S)}\n",
    "index_to_state = {idx: state for state, idx in state_to_index.items()}\n",
    "\n",
    "n_states = len(S)\n",
    "\n",
    "# Construct the transition matrix P under policy f\n",
    "P = np.zeros((n_states, n_states))\n",
    "\n",
    "for k in S:\n",
    "    a = f(k)\n",
    "    k_idx = state_to_index[k]\n",
    "    for j in S:\n",
    "        j_idx = state_to_index[j]\n",
    "        P[k_idx, j_idx] = p_kj(k, j, a)\n",
    "\n",
    "# Transpose P for solving the left eigenvector\n",
    "P_T = P.T\n",
    "\n",
    "# Set up the equations (P_T - I) * pi = 0\n",
    "A = P_T - np.eye(n_states)\n",
    "\n",
    "# Add the normalization constraint sum(pi) = 1\n",
    "A = np.vstack([A, np.ones(n_states)])\n",
    "b = np.zeros(n_states)\n",
    "b = np.append(b, 1)\n",
    "\n",
    "# Solve the linear system\n",
    "pi, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n",
    "\n",
    "# Display the equilibrium probabilities\n",
    "print(\"Equilibrium probabilities π^f_j under policy f:\")\n",
    "for idx, prob in enumerate(pi):\n",
    "    state = index_to_state[idx]\n",
    "    print(f\"State {state}: π^f_{state} = {prob}\")\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```         \n",
    "Equilibrium probabilities π^f_j under policy f:\n",
    "State 0: π^f_0 = 0.5714285714285714\n",
    "State 1: π^f_1 = 0.4285714285714286\n",
    "```\n",
    "\n",
    "This result means that under policy $f$, the system will be in state `0` approximately 57.14% of the time and in state `1` approximately 42.86% of the time in the long run.\n",
    "\n",
    "### **Notes**\n",
    "\n",
    "-   Ensure that your transition probabilities are valid (non-negative and sum to 1 for each state-action pair).\n",
    "-   The method above assumes that the Markov chain induced by policy $f$ is **irreducible** and **aperiodic**, ensuring a unique stationary distribution.\n",
    "-   If your Markov chain has multiple communicating classes or periodic states, you may need to analyze each class separately or use more advanced methods.\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "-   **Step 1**: Define your states, actions, policy, and transition probabilities.\n",
    "-   **Step 2**: Construct the transition matrix $P$ under the given policy.\n",
    "-   **Step 3**: Set up and solve the linear system $(P^T - I) \\pi = 0$ with the normalization constraint $\\sum_j \\pi_j = 1$.\n",
    "-   **Step 4**: Interpret the resulting equilibrium probabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
