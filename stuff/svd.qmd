---
title: "Singular Value Decomposition (SVD) Tutorial"
format: html
editor: visual
jupyter: python3
---

This tutorial combines information from the sources you provided and our conversation history to explain singular value decomposition (SVD), a fundamental matrix factorisation method used in many data-driven applications, especially machine learning.

### What is SVD?

The SVD is a way to break down any matrix into a product of three special matrices. This decomposition reveals important information about the original matrix, like its most important features and its underlying structure.

*   **Data Representation:** Any data you collect can be organised into a matrix, often called a data matrix, represented by the symbol  **X**. If you have *n* different features measured *m* times, your **X** matrix will be *n* x *m*.

*   **SVD Formula:** SVD decomposes this data matrix **X** into three matrices: **UΣV**. This looks like this:

    $X  =  UΣV$

*   **Unitary Matrices:** **U** (*n* x *n*) and **V** (*m* x *m*) are special matrices called *unitary matrices*.  Their columns, called singular vectors, have important geometric properties. Think of them as directions in space that capture the most important variations in your data.

*   **Singular Value Matrix:** **Σ** (*n* x *m*) is a diagonal matrix. This means it has non-zero values only along its main diagonal. These values, called *singular values*, are arranged from largest to smallest. They tell you how much each singular vector contributes to the overall data representation.

*   **Conjugate Transpose:**  The asterisk in  **V** means the *conjugate transpose* of  **V**. This is a mathematical operation that flips the matrix over its diagonal and changes the sign of its imaginary components (if any).

### Why is SVD Important?

*   **Low-Rank Approximations:**  SVD can be used to create simplified versions of your data matrix. By keeping only the largest singular values and their corresponding singular vectors, you get a compressed representation that still captures the most essential information.

*   **Solving Linear Equations:** SVD is a powerful tool for finding solutions to systems of linear equations, even when those systems are tricky or don't have perfect solutions.

*   **Principal Component Analysis (PCA):**  PCA is a technique for finding patterns in high-dimensional data. SVD is the engine behind PCA. By analysing the singular values and vectors, PCA reveals the most descriptive 'directions' in your data.

### Economy SVD: Making Things Efficient

When you have more measurements (*n*) than the number of times you took those measurements (*m*),  **Σ** can be simplified. The *economy SVD* takes advantage of this and gives you the same information as the full SVD but uses smaller matrices, making computations faster and more efficient.

### Computing SVD in Python

Python makes SVD easy with the NumPy library:

```{python}
import numpy as np

# Create a random data matrix 
X = np.random.rand(5, 3) 

# Full SVD
U, S, V = np.linalg.svd(X, full_matrices=True) 

# Economy SVD
Uhat, Shat, Vhat = np.linalg.svd(X, full_matrices=False) 
```

### SVD Calculation: A Step-by-Step Example

Here's a step-by-step example of how to calculate the SVD of a matrix, combining information from the sources you provided and our conversation history:

Let's start with a simple matrix, **A**:

```
A = [[1, 2],
     [3, 4],
     [5, 6]]
```

**Step 1: Calculate the correlation matrices AA\* and A\*A.**

*   **AA\***: This is the row-wise correlation matrix.
*   **A\*A**: This is the column-wise correlation matrix.

In Python:

```{python}
import numpy as np

A = np.array(
              [
                [1, 2],
                [3, 4],
                [5, 6]
              ]
            )

# Calculate the correlation matrices
AAT = np.dot(A, A.transpose())
ATA = np.dot(A.transpose(), A)

print("AA*:\n", AAT)
print("\nA*A:\n", ATA)
```

**Step 2:  Find the eigenvalues and eigenvectors of the correlation matrices.**

*   The eigenvalues represent the squares of the singular values (Σ<sup>2</sup>).
*   The eigenvectors form the columns of the unitary matrices **U** and **V**.

In Python:

```{python}
# Calculate eigenvalues and eigenvectors
eigenvalues_AAT, eigenvectors_AAT = np.linalg.eig(AAT)
eigenvalues_ATA, eigenvectors_ATA = np.linalg.eig(ATA)

print("Eigenvalues of AA*:\n", eigenvalues_AAT)
print("\nEigenvectors of AA*:\n", eigenvectors_AAT)
print("\nEigenvalues of A*A:\n", eigenvalues_ATA)
print("\nEigenvectors of A*A:\n", eigenvectors_ATA)
```

**Step 3: Construct the singular value matrix Σ.**

*   Take the square root of the eigenvalues of either correlation matrix (they'll be the same) to get the singular values.
*   Arrange the singular values in descending order.
*   Create a diagonal matrix with the singular values along the diagonal. 

**Important Note:** This example demonstrates the general mathematical concept of SVD calculation. However, in practice, you almost always use pre-built functions in libraries like NumPy to compute SVD directly as explained in our previous conversation because they are optimised for efficiency and numerical stability.

