---
title: "Unichain assumption"
format: html
editor: visual
jupyter: python3
---

**Assumption 3.3** *There is at least one state x ∈ **X** , such that there is a path from any state to x. If this is the case we call the chain unichain, state x is called recurrent.*

From: Koole, G. (2006). *Lecture notes Stochastic Optimization*.

Visualization of a unichain Markov chain with transient states:

```{mermaid}
stateDiagram
    [*] --> T1
    [*] --> T2
    [*] --> T3
    T1 --> R1
    T2 --> R1
    T3 --> R2
    R1 --> R2
    R2 --> R1
    R1 --> R1
    R2 --> R2: 1
```

**Explanation:**

-   **States:**
    -   **T1, T2, T3:** Transient states. The system may start in any of these states, but once it leaves, it doesn't return.
    -   **R1, R2:** Recurrent states forming a single recurrent class. The system can transition between these states indefinitely.
-   **Transitions:**
    -   Arrows indicate possible transitions between states.
    -   Transient states (T1, T2, T3) have transitions leading to the recurrent states (R1, R2).
    -   Recurrent states (R1, R2) have transitions between each other and to themselves, indicating the possibility of remaining in the same state.

**Key Points:**

-   **Unichain Property:** The chain has a single recurrent class (R1 and R2) and includes transient states (T1, T2, T3).
-   **Transient States:** Once the system leaves a transient state, it doesn't return to it.
-   **Recurrent States:** The system can cycle between R1 and R2 indefinitely, characteristic of a recurrent class.

This diagram illustrates a unichain Markov chain with transient states, where all paths eventually lead to the recurrent class, and within that class, the system can continue indefinitely.

------------------------------------------------------------------------

**Aperiodic** and **periodic** Markov chains.

------------------------------------------------------------------------

## **Understanding Aperiodic and Periodic Markov Chains**

-   **Aperiodic Markov Chain:** A Markov chain is aperiodic if it's possible to return to any state at irregular time steps. Formally, the greatest common divisor (gcd) of the lengths of all possible loops (returns to the same state) is 1.

-   **Periodic Markov Chain:** A Markov chain is periodic if returns to a state can only occur at multiples of some integer greater than 1. The gcd of the return times is greater than 1.

------------------------------------------------------------------------

## **Example 1: Aperiodic Markov Chain**

Let's consider a simple three-state Markov chain where the states are 0, 1, and 2.

### **Transition Matrix**

$$
P = \begin{pmatrix}
0.5 & 0.3 & 0.2 \\
0.1 & 0.6 & 0.3 \\
0.2 & 0.4 & 0.4 \\
\end{pmatrix}
$$

-   All transition probabilities are positive.
-   It's possible to stay in the same state or move to any other state in one step.

------------------------------------------------------------------------

## **Example 2: Periodic Markov Chain**

Consider a two-state Markov chain where the states alternate.

### **Transition Matrix**

$$
P = \begin{pmatrix}
0 & 1 \\
1 & 0 \\
\end{pmatrix}
$$

-   From state 0, you always move to state 1 in the next step.
-   From state 1, you always move to state 0 in the next step.
-   Returns to any state occur only at even time steps.

------------------------------------------------------------------------

## **Python Implementation**

Let's simulate both chains and compare their behaviors.

### **Import Necessary Libraries**

```{python}
import numpy as np
import matplotlib.pyplot as plt
```

### **Define the Markov Chains**

#### **Aperiodic Chain**

```{python}
# States: 0, 1, 2
states_aperiodic = [0, 1, 2]

# Transition Probability Matrix
P_aperiodic = np.array([
    [0.5, 0.3, 0.2],
    [0.1, 0.6, 0.3],
    [0.2, 0.4, 0.4]
])
```

#### **Periodic Chain**

```{python}
# States: 0, 1
states_periodic = [0, 1]

# Transition Probability Matrix
P_periodic = np.array([
    [0, 1],
    [1, 0]
])
```

### **Simulation Function**

We'll define a function to simulate the Markov chain over a number of steps.

```{python}
def simulate_markov_chain(P, states, start_state, num_steps):
    state = start_state
    state_history = [state]
    for _ in range(num_steps):
        state = np.random.choice(states, p=P[state])
        state_history.append(state)
    return state_history
```

### **Simulate the Chains**

#### **Parameters**

```{python}
num_steps = 100
start_state_aperiodic = 0
start_state_periodic = 0
```

#### **Simulate Aperiodic Chain**

```{python}
history_aperiodic = simulate_markov_chain(
    P_aperiodic, states_aperiodic, start_state_aperiodic, num_steps)
```

#### **Simulate Periodic Chain**

```{python}
history_periodic = simulate_markov_chain(
    P_periodic, states_periodic, start_state_periodic, num_steps)
```

### **Visualize the State Visits**

#### **Function to Plot State Visits**

```{python}
def plot_state_visits(state_history, title):
    plt.figure(figsize=(12, 2))
    plt.plot(state_history, 'o-', markersize=4)
    plt.yticks(sorted(set(state_history)))
    plt.xlabel('Step')
    plt.ylabel('State')
    plt.title(title)
    plt.grid(True)
    plt.show()
```

#### **Plot Aperiodic Chain**

```{python}
plot_state_visits(history_aperiodic, 'Aperiodic Markov Chain State Visits')
```

#### **Plot Periodic Chain**

```{python}
plot_state_visits(history_periodic, 'Periodic Markov Chain State Visits')
```

### **Compute State Return Times**

We'll compute the times at which the chain returns to the starting state.

#### **Function to Compute Return Times**

```{python}
def compute_return_times(state_history, target_state):
    return_times = []
    last_visit = None
    for idx, state in enumerate(state_history):
        if state == target_state:
            if last_visit is not None:
                return_times.append(idx - last_visit)
            last_visit = idx
    return return_times
```

#### **Compute and Display Return Times**

```{python}
# Aperiodic Chain
return_times_aperiodic = compute_return_times(history_aperiodic, start_state_aperiodic)
print("Aperiodic Chain Return Times:", return_times_aperiodic)
print("GCD of Return Times (Aperiodic):", np.gcd.reduce(return_times_aperiodic))

# Periodic Chain
return_times_periodic = compute_return_times(history_periodic, start_state_periodic)
print("Periodic Chain Return Times:", return_times_periodic)
print("GCD of Return Times (Periodic):", np.gcd.reduce(return_times_periodic))
```

------------------------------------------------------------------------

## **Results and Explanation**

### **Aperiodic Markov Chain**

-   **State Visits Plot:**

    The plot shows that the chain moves between states 0, 1, and 2 without any regular pattern.

-   **Return Times:**

    The return times to the starting state are irregular, and their gcd is typically 1, indicating aperiodicity.

### **Periodic Markov Chain**

-   **State Visits Plot:**

    The plot shows an alternating pattern between states 0 and 1.

-   **Return Times:**

    The return times to the starting state are consistently 2, 4, 6, etc. The gcd of these times is 2, indicating periodicity with period 2.

------------------------------------------------------------------------

## **Complete Code**

Here's the complete code assembled together:

```{python}
import numpy as np
import matplotlib.pyplot as plt

# Aperiodic Markov Chain
states_aperiodic = [0, 1, 2]
P_aperiodic = np.array([
    [0.5, 0.3, 0.2],
    [0.1, 0.6, 0.3],
    [0.2, 0.4, 0.4]
])

# Periodic Markov Chain
states_periodic = [0, 1]
P_periodic = np.array([
    [0, 1],
    [1, 0]
])

def simulate_markov_chain(P, states, start_state, num_steps):
    state = start_state
    state_history = [state]
    for _ in range(num_steps):
        state = np.random.choice(states, p=P[state])
        state_history.append(state)
    return state_history

def plot_state_visits(state_history, title):
    plt.figure(figsize=(12, 2))
    plt.plot(state_history, 'o-', markersize=4)
    plt.yticks(sorted(set(state_history)))
    plt.xlabel('Step')
    plt.ylabel('State')
    plt.title(title)
    plt.grid(True)
    plt.show()

def compute_return_times(state_history, target_state):
    return_times = []
    last_visit = None
    for idx, state in enumerate(state_history):
        if state == target_state:
            if last_visit is not None:
                return_times.append(idx - last_visit)
            last_visit = idx
    return return_times

# Simulation Parameters
num_steps = 100
start_state_aperiodic = 0
start_state_periodic = 0

# Simulate Chains
history_aperiodic = simulate_markov_chain(
    P_aperiodic, states_aperiodic, start_state_aperiodic, num_steps)

history_periodic = simulate_markov_chain(
    P_periodic, states_periodic, start_state_periodic, num_steps)

# Plot State Visits
plot_state_visits(history_aperiodic, 'Aperiodic Markov Chain State Visits')
plot_state_visits(history_periodic, 'Periodic Markov Chain State Visits')

# Compute Return Times
return_times_aperiodic = compute_return_times(history_aperiodic, start_state_aperiodic)
return_times_periodic = compute_return_times(history_periodic, start_state_periodic)

# Display Return Times and GCD
print("Aperiodic Chain Return Times:", return_times_aperiodic)
if return_times_aperiodic:
    gcd_aperiodic = np.gcd.reduce(return_times_aperiodic)
    print("GCD of Return Times (Aperiodic):", gcd_aperiodic)
else:
    print("No returns to starting state in aperiodic chain.")

print("\nPeriodic Chain Return Times:", return_times_periodic)
if return_times_periodic:
    gcd_periodic = np.gcd.reduce(return_times_periodic)
    print("GCD of Return Times (Periodic):", gcd_periodic)
else:
    print("No returns to starting state in periodic chain.")
```

------------------------------------------------------------------------

## **Interpreting the Output**

### **Aperiodic Chain**

-   **Return Times:** The return times will vary and are not all multiples of a number greater than 1.

-   **GCD:** The gcd of the return times is typically 1, confirming aperiodicity.

### **Periodic Chain**

-   **Return Times:** The return times are consistent and multiples of 2.

-   **GCD:** The gcd of the return times is 2, indicating the chain has a period of 2.

------------------------------------------------------------------------

## **Conclusion**

By simulating both chains, we observe:

-   **Aperiodic Chain:** The chain does not exhibit a fixed cyclic pattern. The state can be revisited at irregular time intervals.

-   **Periodic Chain:** The chain exhibits a fixed cycle, returning to the starting state at regular intervals (every 2 steps).

This simulation helps visualize the difference between aperiodic and periodic Markov chains. In the context of Markov Decision Processes (MDPs), ensuring that the chain is aperiodic is important for the convergence of algorithms and the validity of certain theoretical results, such as the unichain assumption.

```{python}
import numpy as np
import pandas as pd
from scipy.stats import poisson

# Parameters
max_jobs = 8
service_rates = [0, 1, 2]

# Define Poisson arrival probabilities
arrival_rate = 1.5
arrival_probs = {k: poisson.pmf(k, arrival_rate) for k in range(3)}

# Define departure probabilities based on service rate
# For each rate b, f_b(n) where n is the number of completions
departure_probs = {
    0: {1: 0.8, 2: 0.2},
    1: {1: 0.5, 2: 0.5},
    2: {1: 0.2, 2: 0.8}
}

# Define the states as (jobs, service rate)
states = [(x, b) for x in range(max_jobs + 1) for b in service_rates]

# Transition probability dictionary
transition_matrix = []

# Calculate transition probabilities
for (x, b) in states:
    for (x_next, b_next) in states:
        # Calculate transition probability from (x, b) to (x_next, b_next)
        prob = 0
        for arrivals, p_arrival in arrival_probs.items():
            for completions, p_departure in departure_probs[b].items():
                jobs_after_arrivals = min(x + arrivals, max_jobs)
                jobs_after_departures = max(jobs_after_arrivals - completions, 0)
                if jobs_after_departures == x_next and b == b_next:
                    prob += p_arrival * p_departure
        # Store the transition with probability > 0
        if prob > 0:
            transition_matrix.append({
                "from_state": (x, b),
                "to_state": (x_next, b_next),
                "probability": prob
            })

# Convert transition matrix to a DataFrame for better readability
transition_df = pd.DataFrame(transition_matrix)
transition_df = transition_df.sort_values(by=["from_state", "to_state"]).reset_index(drop=True)
transition_df
```

```{python}
import networkx as nx
import matplotlib.pyplot as plt

# Define the state space
# Let's assume x can be 0, 1, 2 (aggregate states)
# b can be 0 or 1 (policy actions)
x_values = [0, 1, 2]
b_values = [0, 1]

states = [(x, b) for x in x_values for b in b_values]

# Define a policy: For simplicity, let's use a deterministic policy
def policy(s):
    x, b = s
    # Example policy: action b is 0 if x is even, 1 if x is odd
    return 0 if x % 2 == 0 else 1

# Build the transition probabilities
# For simplicity, let's define the transition probabilities manually
# Assuming Poisson arrivals and service completions with certain probabilities
# Let's define:
# - Arrival rate λ (new job arrives)
# - Service completion rate μ (job completes)

λ = 0.5  # Arrival probability
μ = 0.5  # Service completion probability

# Initialize the graph
G = nx.DiGraph()

# Add states to the graph
for s in states:
    G.add_node(s)

# Define transitions between states
for s in states:
    x, b = s
    a = policy(s)  # Action determined by the policy
    # Possible transitions:
    # - Arrival: x increases by 1 (if x < max)
    # - Service completion: x decreases by 1 (if x > 0)
    # - No change

    transitions = []

    # Arrival
    if x < max(x_values):
        s_next = (x + 1, policy((x + 1, b)))
        transitions.append((s_next, λ))
        G.add_edge(s, s_next, weight=λ)

    # Service completion
    if x > 0:
        s_next = (x - 1, policy((x - 1, b)))
        transitions.append((s_next, μ))
        G.add_edge(s, s_next, weight=μ)

    # No change
    s_next = (x, b)
    prob_no_change = 1 - λ - μ
    if prob_no_change > 0:
        transitions.append((s_next, prob_no_change))
        G.add_edge(s, s_next, weight=prob_no_change)

# Now, identify aggregate states S(x̃)
aggregate_states = {}
for x in x_values:
    aggregate_states[x] = [s for s in states if s[0] == x]

# Visualize the Markov chain
pos = nx.spring_layout(G)

# Draw nodes with different colors based on aggregate states
colors = ['red', 'green', 'blue']
node_colors = []
for s in G.nodes():
    x, b = s
    color = colors[x_values.index(x)]
    node_colors.append(color)

nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=500)
nx.draw_networkx_labels(G, pos, labels={s: f"{s}" for s in G.nodes()}, font_size=8)

# Draw edges with labels (transition probabilities)
edge_labels = nx.get_edge_attributes(G, 'weight')
nx.draw_networkx_edges(G, pos, arrows=True)
nx.draw_networkx_edge_labels(G, pos, edge_labels={(u, v): f"{d['weight']:.1f}" for u, v, d in G.edges(data=True)}, font_size=6)

plt.title("Markov Chain Visualization with Aggregate States")
plt.axis('off')
plt.show()

# Identify recurrent classes and transient states
# For simplicity, we'll use strongly connected components
# In a unichain, all states communicate, forming a single recurrent class
components = list(nx.strongly_connected_components(G))

print("Strongly connected components (potential recurrent classes):")
for i, comp in enumerate(components):
    print(f"Component {i+1}: {comp}")

```

```{python}
from graphviz import Digraph

# Create a new directed graph
dot = Digraph(comment='Unichain', format='png')
dot.attr(rankdir='LR', nodesep='1.5', ranksep='2.0')  # Increased nodesep and ranksep for more space between nodes

# Define nodes
dot.attr('node', shape='circle')
dot.node('A', 'x̂, b0')
dot.node('B', 'x̂, b1')
dot.node('C', 'x̂, b2')
dot.attr('node', shape='doublecircle')
dot.node('D', 'x ≠ x̂')

# Define edges
dot.edge('A', 'A', label='b0')
dot.edge('B', 'A', label='b0')
dot.edge('C', 'A', label='b0')
dot.edge('A', 'D', style='dashed', label='b0')
dot.edge('D', 'A', style='dashed', label='b0')
dot.edge('B', 'D', style='dashed', label='b0')
dot.edge('C', 'D', style='dashed', label='b0')
dot.edge('D', 'D', label='b0')

# Render the diagram (this will save it to a file)
dot.render('unichain_always_b0_diagram')
```

![](unichain_always_b0_diagram.png)

```{python}
# Create a new directed graph
dot = Digraph(comment='Unichain', format='png')
dot.attr(rankdir='LR', nodesep='1.5', ranksep='2.0')  # Increased nodesep and ranksep for more space between nodes

# Define nodes
dot.attr('node', shape='circle')
dot.node('A', 'x̂, b0')
dot.node('B', 'x̂, b1')
dot.node('C', 'x̂, b2')
dot.attr('node', shape='doublecircle')
dot.node('D', 'x ≠ x̂')

# Define edges
dot.edge('A', 'A', label='b0')
dot.edge('B', 'B', label='b1')
dot.edge('C', 'C', label='b2')
dot.edge('A', 'D', style='dashed', label='b0')
dot.edge('D', 'A', style='dashed', label='b0')
dot.edge('B', 'D', style='dashed', label='b1')
dot.edge('D', 'B', style='dashed', label='b1')
dot.edge('C', 'D', style='dashed', label='b2')
dot.edge('D', 'C', style='dashed', label='b2')
dot.edge('D', 'D')

# Render the diagram (this will save it to a file)
dot.render('unichain_b0_b1_b2_diagram')
```

![](unichain_b0_b1_b2_diagram.png)
